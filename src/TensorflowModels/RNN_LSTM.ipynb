{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../../datasets/final_dataset.csv\")\n",
    "\n",
    "# First split: train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"Text\"], dataset[\"Label\"], test_size=0.2, random_state=42, stratify=dataset[\"Label\"]\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=100)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - accuracy: 0.5262 - loss: 1.0683 - val_accuracy: 0.6579 - val_loss: 0.9509 - learning_rate: 2.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.5152 - loss: 1.0002 - val_accuracy: 0.7381 - val_loss: 0.9209 - learning_rate: 2.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.5674 - loss: 0.9013 - val_accuracy: 0.7550 - val_loss: 0.8910 - learning_rate: 2.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.6033 - loss: 0.8635 - val_accuracy: 0.8243 - val_loss: 0.8613 - learning_rate: 2.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.6235 - loss: 0.8037 - val_accuracy: 0.8737 - val_loss: 0.8239 - learning_rate: 2.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.6619 - loss: 0.7698 - val_accuracy: 0.8937 - val_loss: 0.7774 - learning_rate: 2.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.6902 - loss: 0.7348 - val_accuracy: 0.8937 - val_loss: 0.7298 - learning_rate: 2.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.7233 - loss: 0.6865 - val_accuracy: 0.8983 - val_loss: 0.6729 - learning_rate: 2.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - accuracy: 0.7453 - loss: 0.6379 - val_accuracy: 0.9076 - val_loss: 0.6236 - learning_rate: 2.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.7466 - loss: 0.6308 - val_accuracy: 0.9307 - val_loss: 0.5716 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64),  \n",
    "\n",
    "    LSTM(16, activation=\"tanh\", return_sequences=True, \n",
    "         kernel_regularizer=l2(0.003), recurrent_dropout=0.5),  # Increased L2 and Dropout\n",
    "    Dropout(0.6),\n",
    "\n",
    "    LSTM(8, kernel_regularizer=l2(0.003), recurrent_dropout=0.5, return_sequences=False),\n",
    "    Dropout(0.6),\n",
    "\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(16, activation=\"relu\", kernel_regularizer=l2(0.003)),  # Increased L2\n",
    "    Dropout(0.7),  # Increased Dropout\n",
    "\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.0002)  # Further reduced learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping (More aggressive stopping)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,  # Decreased patience for early stopping\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,  # More aggressive reduction\n",
    "    patience=1,\n",
    "    min_lr=0.00005\n",
    ")\n",
    "\n",
    "# Train with adjusted dropout, regularization, and patience for early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  # Keep more epochs to allow gradual improvement\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9215 - loss: 0.5764\n",
      "Test accuracy: 0.9248\n",
      "\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9380 - loss: 0.5671\n",
      "Validation accuracy: 0.9307\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Accuracy: 0.7125\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "5   Human    0.275818           AI\n",
      "6   Human    0.263894           AI\n",
      "9   Human    0.374017           AI\n",
      "11  Human    0.460675           AI\n",
      "12  Human    0.322826           AI\n",
      "13  Human    0.232764           AI\n",
      "22  Human    0.296325           AI\n",
      "24  Human    0.373004           AI\n",
      "28  Human    0.430029           AI\n",
      "32     AI    0.508301        Human\n",
      "38  Human    0.469014           AI\n",
      "39  Human    0.414330           AI\n",
      "42  Human    0.371303           AI\n",
      "48     AI    0.661211        Human\n",
      "50     AI    0.518896        Human\n",
      "51  Human    0.323387           AI\n",
      "54     AI    0.564022        Human\n",
      "57  Human    0.381043           AI\n",
      "59  Human    0.490460           AI\n",
      "67  Human    0.453814           AI\n",
      "69  Human    0.443328           AI\n",
      "75  Human    0.496791           AI\n",
      "79  Human    0.344047           AI\n"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "new_data = pd.read_csv(\"../../datasets/validation_dataset.csv\", delimiter=\";\")\n",
    "\n",
    "# Tokenize and pad the new data (using the same tokenizer you trained on)\n",
    "X_new_seq = tokenizer.texts_to_sequences(new_data[\"Text\"])\n",
    "X_new = pad_sequences(X_new_seq, maxlen=100)  # Ensure maxlen is consistent with your training data\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new).flatten()\n",
    "\n",
    "# Convert predictions to labels based on threshold\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame with predictions\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "# Load the ground truth labels (from the same dataset)\n",
    "# Since the labels are in the 'Label' column, we'll compare them with predictions.\n",
    "ground_truth = new_data[\"Label\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Merge predictions with ground truth for comparison\n",
    "comparison_df = output_df.copy()\n",
    "comparison_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
