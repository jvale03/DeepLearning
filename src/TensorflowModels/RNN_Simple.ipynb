{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(144)\n",
    "random.seed(144)\n",
    "tf.random.set_seed(144)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../../datasets/final_dataset.csv\")\n",
    "\n",
    "# First split: train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"Text\"], dataset[\"Label\"], test_size=0.2, random_state=144, stratify=dataset[\"Label\"]\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=144, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=100)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tumab\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.5697 - loss: 0.7204 - val_accuracy: 0.7381 - val_loss: 0.6405\n",
      "Epoch 2/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7632 - loss: 0.5712 - val_accuracy: 0.9091 - val_loss: 0.3309\n",
      "Epoch 3/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9462 - loss: 0.1945 - val_accuracy: 0.9461 - val_loss: 0.1856\n",
      "Epoch 4/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9488 - loss: 0.3272 - val_accuracy: 0.8552 - val_loss: 0.3150\n",
      "Epoch 5/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9139 - loss: 0.2146 - val_accuracy: 0.9168 - val_loss: 0.2074\n",
      "Epoch 6/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9841 - loss: 0.1216 - val_accuracy: 0.9337 - val_loss: 0.1711\n",
      "Epoch 7/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9969 - loss: 0.0820 - val_accuracy: 0.9368 - val_loss: 0.1588\n",
      "Epoch 8/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0565 - val_accuracy: 0.9291 - val_loss: 0.1549\n",
      "Epoch 9/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9964 - loss: 0.0458 - val_accuracy: 0.9307 - val_loss: 0.2533\n",
      "Epoch 10/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9494 - loss: 0.1479 - val_accuracy: 0.8860 - val_loss: 0.2792\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=50, input_length=100),\n",
    "    SimpleRNN(64, activation=\"relu\"),\n",
    "    \n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8593 - loss: 0.3401\n",
      "Test accuracy: 0.8705\n",
      "\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8876 - loss: 0.2670\n",
      "Validation accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Accuracy: 0.6500\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "5   Human    0.007376           AI\n",
      "6   Human    0.004963           AI\n",
      "9   Human    0.063584           AI\n",
      "10  Human    0.259195           AI\n",
      "11  Human    0.177374           AI\n",
      "12  Human    0.029857           AI\n",
      "13  Human    0.001452           AI\n",
      "18  Human    0.183009           AI\n",
      "22  Human    0.021208           AI\n",
      "24  Human    0.133533           AI\n",
      "25     AI    0.968817        Human\n",
      "33  Human    0.231999           AI\n",
      "34  Human    0.198664           AI\n",
      "36     AI    0.568051        Human\n",
      "37  Human    0.385023           AI\n",
      "38  Human    0.326807           AI\n",
      "39  Human    0.251432           AI\n",
      "42  Human    0.042118           AI\n",
      "43  Human    0.401822           AI\n",
      "48     AI    0.776427        Human\n",
      "51  Human    0.013412           AI\n",
      "57  Human    0.143792           AI\n",
      "59  Human    0.466122           AI\n",
      "61  Human    0.399194           AI\n",
      "69  Human    0.135811           AI\n",
      "71  Human    0.171110           AI\n",
      "75  Human    0.453890           AI\n",
      "79  Human    0.031945           AI\n"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "new_data = pd.read_csv(\"../../datasets/validation_dataset.csv\", delimiter=\";\")\n",
    "\n",
    "# Tokenize and pad the new data (using the same tokenizer you trained on)\n",
    "X_new_seq = tokenizer.texts_to_sequences(new_data[\"Text\"])\n",
    "X_new = pad_sequences(X_new_seq, maxlen=100)  # Ensure maxlen is consistent with your training data\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new).flatten()\n",
    "\n",
    "# Convert predictions to labels based on threshold\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame with predictions\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "# Load the ground truth labels (from the same dataset)\n",
    "# Since the labels are in the 'Label' column, we'll compare them with predictions.\n",
    "ground_truth = new_data[\"Label\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Merge predictions with ground truth for comparison\n",
    "comparison_df = output_df.copy()\n",
    "comparison_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sentences Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Sentence: \n",
      "The human immune system is the body’s defense mechanism against harmful invaders like bacteria, viruses, fungi, and parasites. It consists of two main parts: the innate immune system and the adaptive immune system. The innate immune system provides a rapid, non-specific response to infections, using barriers like the skin and white blood cells. The adaptive immune system, on the other hand, targets specific pathogens and creates lasting immunity. Key components of the adaptive system include T cells and B cells, which remember previous infections and help the body fight them more effectively in the future. A healthy immune system is essential for preventing diseases, and its dysfunction can lead to conditions like autoimmune disorders or immunodeficiency.\n",
      "\n",
      "\n",
      "Predicted Label: AI\n",
      "Prediction Score: 0.6756\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded sentence\n",
    "hardcoded_sentence = \"\"\"\n",
    "The human immune system is the body’s defense mechanism against harmful invaders like bacteria, viruses, fungi, and parasites. It consists of two main parts: the innate immune system and the adaptive immune system. The innate immune system provides a rapid, non-specific response to infections, using barriers like the skin and white blood cells. The adaptive immune system, on the other hand, targets specific pathogens and creates lasting immunity. Key components of the adaptive system include T cells and B cells, which remember previous infections and help the body fight them more effectively in the future. A healthy immune system is essential for preventing diseases, and its dysfunction can lead to conditions like autoimmune disorders or immunodeficiency.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and pad the sentence\n",
    "X_hardcoded_seq = tokenizer.texts_to_sequences([hardcoded_sentence])  # Use the same tokenizer\n",
    "X_hardcoded = pad_sequences(X_hardcoded_seq, maxlen=100)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(X_hardcoded).flatten()[0] \n",
    "\n",
    "# Convert prediction to label\n",
    "label = \"AI\" if prediction > 0.5 else \"Human\"\n",
    "\n",
    "# Print result\n",
    "print(f\"Sentence: {hardcoded_sentence}\")\n",
    "print(f\"Predicted Label: {label}\")\n",
    "print(f\"Prediction Score: {prediction:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
