{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import sentence-transformers for easy BERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for embeddings if it doesn't exist\n",
    "embeddings_dir = \"bert_embeddings\"\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "# Filepaths for saved embeddings\n",
    "train_emb_path = os.path.join(embeddings_dir, \"train_embeddings.npy\")\n",
    "val_emb_path = os.path.join(embeddings_dir, \"val_embeddings.npy\")\n",
    "test_emb_path = os.path.join(embeddings_dir, \"test_embeddings.npy\")\n",
    "scaler_path = os.path.join(embeddings_dir, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../../datasets/final_dataset.csv\")\n",
    "\n",
    "# First split: train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"Text\"], dataset[\"Label\"], test_size=0.2, random_state=42, stratify=dataset[\"Label\"]\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed BERT embeddings...\n",
      "Loaded embeddings: Train shape (2593, 384), Val shape (649, 384), Test shape (811, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained BERT-based sentence transformer model\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Smaller BERT model variant\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if (os.path.exists(train_emb_path) and \n",
    "    os.path.exists(val_emb_path) and \n",
    "    os.path.exists(test_emb_path) and\n",
    "    os.path.exists(scaler_path)):\n",
    "    \n",
    "    print(\"Loading pre-computed BERT embeddings...\")\n",
    "    X_train_bert = np.load(train_emb_path)\n",
    "    X_val_bert = np.load(val_emb_path)\n",
    "    X_test_bert = np.load(test_emb_path)\n",
    "    \n",
    "    # Load the scaler\n",
    "    import pickle\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded embeddings: Train shape {X_train_bert.shape}, Val shape {X_val_bert.shape}, Test shape {X_test_bert.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Computing BERT embeddings (this may take a while)...\")\n",
    "\n",
    "    print(\"Extracting BERT embeddings for training set...\")\n",
    "    X_train_bert = bert_model.encode(train_texts.tolist(), \n",
    "                                    show_progress_bar=True, \n",
    "                                    batch_size=32)\n",
    "\n",
    "    print(\"Extracting BERT embeddings for validation set...\")\n",
    "    X_val_bert = bert_model.encode(val_texts.tolist(), \n",
    "                                  show_progress_bar=True, \n",
    "                                  batch_size=32)\n",
    "\n",
    "    print(\"Extracting BERT embeddings for test set...\")\n",
    "    X_test_bert = bert_model.encode(test_texts.tolist(), \n",
    "                                   show_progress_bar=True, \n",
    "                                   batch_size=32)\n",
    "    \n",
    "    # Save the embeddings\n",
    "    print(\"Saving embeddings to disk for future use...\")\n",
    "    np.save(train_emb_path, X_train_bert)\n",
    "    np.save(val_emb_path, X_val_bert)\n",
    "    np.save(test_emb_path, X_test_bert)\n",
    "    \n",
    "    # Create and save the scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_bert)\n",
    "    \n",
    "    import pickle\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(\"Embeddings and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the embeddings (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bert)\n",
    "X_val_scaled = scaler.transform(X_val_bert)\n",
    "X_test_scaled = scaler.transform(X_test_bert)\n",
    "\n",
    "# Reshape embeddings for LSTM: (samples, time steps, features)\n",
    "# We'll reshape our embeddings into a sequence format for LSTM\n",
    "embedding_dim = X_train_scaled.shape[1]  # Should be 384 for this model\n",
    "time_steps = 4  # Split embedding into 4 parts to create a sequence\n",
    "features = embedding_dim // time_steps\n",
    "\n",
    "# Reshape 2D embeddings to 3D for LSTM: (samples, time_steps, features)\n",
    "X_train = X_train_scaled.reshape(-1, time_steps, features)\n",
    "X_val = X_val_scaled.reshape(-1, time_steps, features)\n",
    "X_test = X_test_scaled.reshape(-1, time_steps, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\miniconda3\\envs\\daa\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build model with your original LSTM architecture\n",
    "model = Sequential([\n",
    "    # No embedding layer needed since we already have BERT embeddings\n",
    "    \n",
    "    LSTM(16, activation=\"tanh\", return_sequences=True, \n",
    "         kernel_regularizer=l2(0.003), recurrent_dropout=0.5,\n",
    "         input_shape=(time_steps, features)),  # Specify input shape\n",
    "    Dropout(0.4),\n",
    "\n",
    "    LSTM(8, kernel_regularizer=l2(0.003), recurrent_dropout=0.5, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(16, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "learning_rate = 0.0005\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Compile with appropriate learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Add callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=1,\n",
    "    min_lr=0.00005\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.5069 - loss: 1.0848 - val_accuracy: 0.5193 - val_loss: 0.9858 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4846 - loss: 1.0372 - val_accuracy: 0.5270 - val_loss: 0.9611 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5020 - loss: 0.9980 - val_accuracy: 0.5362 - val_loss: 0.9366 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5087 - loss: 0.9658 - val_accuracy: 0.5562 - val_loss: 0.9129 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5394 - loss: 0.9251 - val_accuracy: 0.5778 - val_loss: 0.8901 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5373 - loss: 0.9046 - val_accuracy: 0.6148 - val_loss: 0.8656 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5517 - loss: 0.8830 - val_accuracy: 0.6317 - val_loss: 0.8412 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5568 - loss: 0.8600 - val_accuracy: 0.6641 - val_loss: 0.8139 - learning_rate: 5.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5864 - loss: 0.8312 - val_accuracy: 0.6949 - val_loss: 0.7840 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6089 - loss: 0.7997 - val_accuracy: 0.7334 - val_loss: 0.7495 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6530 - loss: 0.7603 - val_accuracy: 0.7473 - val_loss: 0.7105 - learning_rate: 5.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6847 - loss: 0.7260 - val_accuracy: 0.7612 - val_loss: 0.6734 - learning_rate: 5.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7130 - loss: 0.6947 - val_accuracy: 0.7904 - val_loss: 0.6367 - learning_rate: 5.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7354 - loss: 0.6595 - val_accuracy: 0.7966 - val_loss: 0.6050 - learning_rate: 5.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7595 - loss: 0.6264 - val_accuracy: 0.8089 - val_loss: 0.5799 - learning_rate: 5.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7742 - loss: 0.5978 - val_accuracy: 0.8012 - val_loss: 0.5530 - learning_rate: 5.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7805 - loss: 0.5747 - val_accuracy: 0.8197 - val_loss: 0.5355 - learning_rate: 5.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7920 - loss: 0.5652 - val_accuracy: 0.8336 - val_loss: 0.5198 - learning_rate: 5.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7910 - loss: 0.5663 - val_accuracy: 0.8336 - val_loss: 0.5052 - learning_rate: 5.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8248 - loss: 0.5152 - val_accuracy: 0.8336 - val_loss: 0.4908 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8116 - loss: 0.5139\n",
      "Test accuracy: 0.8187\n",
      "\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8352 - loss: 0.4801\n",
      "Validation accuracy: 0.8336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benchmark data...\n",
      "Making predictions on benchmark data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871ms/step\n",
      "Benchmark accuracy: 0.6333\n",
      "\n",
      "Number of misclassified samples: 11\n",
      "       ID Label_predicted  Prediction Label_actual\n",
      "0    D1-1           Human    0.254965        Human\n",
      "1    D1-2              AI    0.631633           AI\n",
      "2    D1-3           Human    0.241493        Human\n",
      "3    D1-4           Human    0.142752           AI\n",
      "4    D1-5              AI    0.838111        Human\n",
      "5    D1-6              AI    0.967630           AI\n",
      "6    D1-7           Human    0.299215        Human\n",
      "7    D1-8           Human    0.433918           AI\n",
      "8    D1-9              AI    0.717566        Human\n",
      "9   D1-10              AI    0.850823           AI\n",
      "10  D1-11              AI    0.979280        Human\n",
      "11  D1-12              AI    0.965557           AI\n",
      "12  D1-13           Human    0.120815        Human\n",
      "13  D1-14              AI    0.563560           AI\n",
      "14  D1-15              AI    0.935525        Human\n",
      "15  D1-16              AI    0.878905           AI\n",
      "16  D1-17              AI    0.569693        Human\n",
      "17  D1-18              AI    0.618604           AI\n",
      "18  D1-19              AI    0.726958        Human\n",
      "19  D1-20              AI    0.622893           AI\n",
      "20  D1-21              AI    0.828546        Human\n",
      "21  D1-22              AI    0.706558           AI\n",
      "22  D1-23              AI    0.917543        Human\n",
      "23  D1-24              AI    0.594410           AI\n",
      "24  D1-25           Human    0.208002        Human\n",
      "25  D1-26              AI    0.905730           AI\n",
      "26  D1-27           Human    0.281194        Human\n",
      "27  D1-28           Human    0.425048           AI\n",
      "28  D1-29           Human    0.463390        Human\n",
      "29  D1-30              AI    0.706715           AI\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to process new data and make predictions\n",
    "def predict_on_new_data(new_texts):\n",
    "    # Extract BERT embeddings\n",
    "    new_embeddings = bert_model.encode(new_texts, show_progress_bar=True, batch_size=32)\n",
    "    # Scale embeddings\n",
    "    new_embeddings_scaled = scaler.transform(new_embeddings)\n",
    "    # Reshape for LSTM\n",
    "    new_embeddings_reshaped = new_embeddings_scaled.reshape(-1, time_steps, features)\n",
    "    # Predict\n",
    "    return model.predict(new_embeddings_reshaped).flatten()\n",
    "\n",
    "# Benchmarking\n",
    "print(\"Loading benchmark data...\")\n",
    "new_data = pd.read_csv(\"../../datasets/dataset1_inputs.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions on benchmark data...\")\n",
    "predictions = predict_on_new_data(new_data[\"Text\"].tolist())\n",
    "\n",
    "# Convert predictions to labels\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\"ID\": new_data[\"ID\"], \"Label\": labels, \"Prediction\": predictions})\n",
    "\n",
    "# Load the correct labels\n",
    "ground_truth = pd.read_csv(\"../../datasets/dataset1_outputs.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Merge predictions with ground truth\n",
    "comparison_df = output_df.merge(ground_truth, on=\"ID\", suffixes=(\"_predicted\", \"_actual\"))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (comparison_df[\"Label_predicted\"] == comparison_df[\"Label_actual\"]).mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Benchmark accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label_predicted\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nNumber of misclassified samples:\", len(misclassified))\n",
    "\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
