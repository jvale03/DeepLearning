{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../../datasets/final_dataset.csv\")\n",
    "\n",
    "# First split: train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"Text\"], dataset[\"Label\"], test_size=0.2, random_state=42, stratify=dataset[\"Label\"]\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=100)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5100 - loss: 1.5501 - val_accuracy: 0.4823 - val_loss: 1.0940 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4911 - loss: 1.3445 - val_accuracy: 0.4823 - val_loss: 1.0767 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5102 - loss: 1.2337 - val_accuracy: 0.4823 - val_loss: 1.0605 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5329 - loss: 1.1322 - val_accuracy: 0.4823 - val_loss: 1.0459 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5250 - loss: 1.1111 - val_accuracy: 0.4823 - val_loss: 1.0324 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5654 - loss: 1.0402 - val_accuracy: 0.4823 - val_loss: 1.0192 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5633 - loss: 1.0424 - val_accuracy: 0.4869 - val_loss: 1.0058 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5911 - loss: 0.9620 - val_accuracy: 0.5254 - val_loss: 0.9893 - learning_rate: 5.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5750 - loss: 0.9685 - val_accuracy: 0.6487 - val_loss: 0.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5960 - loss: 0.9293 - val_accuracy: 0.6549 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6309 - loss: 0.9148 - val_accuracy: 0.7827 - val_loss: 0.8915 - learning_rate: 5.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6608 - loss: 0.8768 - val_accuracy: 0.8475 - val_loss: 0.8397 - learning_rate: 5.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7083 - loss: 0.8487 - val_accuracy: 0.8444 - val_loss: 0.8193 - learning_rate: 5.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7133 - loss: 0.8192 - val_accuracy: 0.8998 - val_loss: 0.7526 - learning_rate: 5.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7525 - loss: 0.7858 - val_accuracy: 0.9137 - val_loss: 0.7214 - learning_rate: 5.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7730 - loss: 0.7617 - val_accuracy: 0.9183 - val_loss: 0.6946 - learning_rate: 5.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7903 - loss: 0.7377 - val_accuracy: 0.9322 - val_loss: 0.6508 - learning_rate: 5.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8312 - loss: 0.6891 - val_accuracy: 0.9707 - val_loss: 0.5810 - learning_rate: 5.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8318 - loss: 0.6628 - val_accuracy: 0.9738 - val_loss: 0.5591 - learning_rate: 5.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8443 - loss: 0.6427 - val_accuracy: 0.9676 - val_loss: 0.5312 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # Keep the Embedding layer since we're working with text\n",
    "    Embedding(input_dim=10000, output_dim=64),\n",
    "    \n",
    "    # Add a GlobalAveragePooling1D layer to convert sequences to fixed-size vectors\n",
    "    # This replaces the recurrent nature of LSTMs\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    # First Dense layer (larger than LSTM output)\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.6),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Second Dense layer\n",
    "    Dense(32, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.6),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Third Dense layer (similar to your original model)\n",
    "    Dense(16, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(8, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    # Output layer stays the same\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "learning_rate = 0.0005\n",
    "epoch = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Compile with a lower learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)  # Further reduced learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Early Stopping (More aggressive stopping)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,  # Decreased patience for early stopping\n",
    "    restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,  # More aggressive reduction\n",
    "    patience=1,\n",
    "    min_lr=0.00005\n",
    ")\n",
    "# Train with adjusted dropout, regularization, and patience for early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epoch,  # Keep more epochs to allow gradual improvement\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9605 - loss: 0.5353 \n",
      "Test accuracy: 0.9618\n",
      "\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9680 - loss: 0.5278 \n",
      "Validation accuracy: 0.9676\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Accuracy: 0.6500\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "2   Human    0.402408           AI\n",
      "5   Human    0.011797           AI\n",
      "6   Human    0.244139           AI\n",
      "9   Human    0.018257           AI\n",
      "11  Human    0.014018           AI\n",
      "12  Human    0.056686           AI\n",
      "13  Human    0.192245           AI\n",
      "16  Human    0.364192           AI\n",
      "18  Human    0.337724           AI\n",
      "22  Human    0.059323           AI\n",
      "24  Human    0.052490           AI\n",
      "28  Human    0.038217           AI\n",
      "32     AI    0.576500        Human\n",
      "35  Human    0.317834           AI\n",
      "37  Human    0.424016           AI\n",
      "38  Human    0.471474           AI\n",
      "39  Human    0.478572           AI\n",
      "42  Human    0.463638           AI\n",
      "48     AI    0.576500        Human\n",
      "50     AI    0.576500        Human\n",
      "53  Human    0.005264           AI\n",
      "57  Human    0.188895           AI\n",
      "59  Human    0.031762           AI\n",
      "61  Human    0.287380           AI\n",
      "67  Human    0.054340           AI\n",
      "72     AI    0.585553        Human\n",
      "77  Human    0.470588           AI\n",
      "79  Human    0.092083           AI\n"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "new_data = pd.read_csv(\"../../datasets/validation_dataset.csv\", delimiter=\";\")\n",
    "\n",
    "# Tokenize and pad the new data (using the same tokenizer you trained on)\n",
    "X_new_seq = tokenizer.texts_to_sequences(new_data[\"Text\"])\n",
    "X_new = pad_sequences(X_new_seq, maxlen=100)  # Ensure maxlen is consistent with your training data\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new).flatten()\n",
    "\n",
    "# Convert predictions to labels based on threshold\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame with predictions\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "# Load the ground truth labels (from the same dataset)\n",
    "# Since the labels are in the 'Label' column, we'll compare them with predictions.\n",
    "ground_truth = new_data[\"Label\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Merge predictions with ground truth for comparison\n",
    "comparison_df = output_df.copy()\n",
    "comparison_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These other combinations also provide good results\n",
    "### i.e., >= 0.6 for the professor data\n",
    "\n",
    "# Epochs: 10, Batch Size: 16, Learning Rate: 0.001\n",
    "# Epochs: 10, Batch Size: 16, Learning Rate: 0.0002\n",
    "# Epochs: 10, Batch Size: 32, Learning Rate: 0.0005\n",
    "# Epochs: 20, Batch Size: 16, Learning Rate: 0.0002\n",
    "# Epochs: 20, Batch Size: 64, Learning Rate: 0.0002"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
