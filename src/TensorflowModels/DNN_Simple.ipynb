{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../../datasets/final_dataset.csv\")\n",
    "\n",
    "# First split: train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"Text\"], dataset[\"Label\"], test_size=0.2, random_state=42, stratify=dataset[\"Label\"]\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=100)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5100 - loss: 1.5501 - val_accuracy: 0.4823 - val_loss: 1.0940 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4911 - loss: 1.3445 - val_accuracy: 0.4823 - val_loss: 1.0767 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5102 - loss: 1.2337 - val_accuracy: 0.4823 - val_loss: 1.0605 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5329 - loss: 1.1322 - val_accuracy: 0.4823 - val_loss: 1.0459 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5250 - loss: 1.1111 - val_accuracy: 0.4823 - val_loss: 1.0324 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5654 - loss: 1.0402 - val_accuracy: 0.4823 - val_loss: 1.0192 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5633 - loss: 1.0424 - val_accuracy: 0.4869 - val_loss: 1.0058 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5911 - loss: 0.9620 - val_accuracy: 0.5254 - val_loss: 0.9893 - learning_rate: 5.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5750 - loss: 0.9685 - val_accuracy: 0.6487 - val_loss: 0.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5960 - loss: 0.9293 - val_accuracy: 0.6549 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6309 - loss: 0.9148 - val_accuracy: 0.7827 - val_loss: 0.8915 - learning_rate: 5.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6608 - loss: 0.8768 - val_accuracy: 0.8475 - val_loss: 0.8397 - learning_rate: 5.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7083 - loss: 0.8487 - val_accuracy: 0.8444 - val_loss: 0.8193 - learning_rate: 5.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7133 - loss: 0.8192 - val_accuracy: 0.8998 - val_loss: 0.7526 - learning_rate: 5.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7525 - loss: 0.7858 - val_accuracy: 0.9137 - val_loss: 0.7214 - learning_rate: 5.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7730 - loss: 0.7617 - val_accuracy: 0.9183 - val_loss: 0.6946 - learning_rate: 5.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7903 - loss: 0.7377 - val_accuracy: 0.9322 - val_loss: 0.6508 - learning_rate: 5.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8312 - loss: 0.6891 - val_accuracy: 0.9707 - val_loss: 0.5810 - learning_rate: 5.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8318 - loss: 0.6628 - val_accuracy: 0.9738 - val_loss: 0.5591 - learning_rate: 5.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8443 - loss: 0.6427 - val_accuracy: 0.9676 - val_loss: 0.5312 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # Keep the Embedding layer since we're working with text\n",
    "    Embedding(input_dim=10000, output_dim=64),\n",
    "    \n",
    "    # Add a GlobalAveragePooling1D layer to convert sequences to fixed-size vectors\n",
    "    # This replaces the recurrent nature of LSTMs\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    # First Dense layer (larger than LSTM output)\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.6),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Second Dense layer\n",
    "    Dense(32, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.6),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Third Dense layer (similar to your original model)\n",
    "    Dense(16, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(8, activation=\"relu\", kernel_regularizer=l2(0.003)),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    # Output layer stays the same\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "learning_rate = 0.0005\n",
    "epoch = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Compile with a lower learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)  # Further reduced learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Early Stopping (More aggressive stopping)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,  # Decreased patience for early stopping\n",
    "    restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,  # More aggressive reduction\n",
    "    patience=1,\n",
    "    min_lr=0.00005\n",
    ")\n",
    "# Train with adjusted dropout, regularization, and patience for early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epoch,  # Keep more epochs to allow gradual improvement\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9605 - loss: 0.5353 \n",
      "Test accuracy: 0.9618\n",
      "\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.5278 \n",
      "Validation accuracy: 0.9676\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Accuracy: 0.7000\n",
      "\n",
      "Misclassified Samples:\n",
      "       ID Label_predicted  Prediction Label_actual\n",
      "0    D1-1              AI    0.576500        Human\n",
      "3    D1-4           Human    0.005264           AI\n",
      "7    D1-8           Human    0.188895           AI\n",
      "9   D1-10           Human    0.031762           AI\n",
      "11  D1-12           Human    0.287380           AI\n",
      "17  D1-18           Human    0.054340           AI\n",
      "22  D1-23              AI    0.585553        Human\n",
      "27  D1-28           Human    0.470588           AI\n",
      "29  D1-30           Human    0.092083           AI\n",
      "\n",
      "-----------------------------------\n",
      "All Samples:\n",
      "       ID Label_predicted  Prediction Label_actual\n",
      "0    D1-1              AI    0.576500        Human\n",
      "1    D1-2              AI    0.558626           AI\n",
      "2    D1-3           Human    0.076806        Human\n",
      "3    D1-4           Human    0.005264           AI\n",
      "4    D1-5           Human    0.494074        Human\n",
      "5    D1-6              AI    0.518217           AI\n",
      "6    D1-7           Human    0.132446        Human\n",
      "7    D1-8           Human    0.188895           AI\n",
      "8    D1-9           Human    0.093922        Human\n",
      "9   D1-10           Human    0.031762           AI\n",
      "10  D1-11           Human    0.100483        Human\n",
      "11  D1-12           Human    0.287380           AI\n",
      "12  D1-13           Human    0.016512        Human\n",
      "13  D1-14              AI    0.576500           AI\n",
      "14  D1-15           Human    0.002766        Human\n",
      "15  D1-16              AI    0.567604           AI\n",
      "16  D1-17           Human    0.250359        Human\n",
      "17  D1-18           Human    0.054340           AI\n",
      "18  D1-19           Human    0.018849        Human\n",
      "19  D1-20              AI    0.543088           AI\n",
      "20  D1-21           Human    0.223056        Human\n",
      "21  D1-22              AI    0.568740           AI\n",
      "22  D1-23              AI    0.585553        Human\n",
      "23  D1-24              AI    0.573386           AI\n",
      "24  D1-25           Human    0.001224        Human\n",
      "25  D1-26              AI    0.518968           AI\n",
      "26  D1-27           Human    0.293524        Human\n",
      "27  D1-28           Human    0.470588           AI\n",
      "28  D1-29           Human    0.460235        Human\n",
      "29  D1-30           Human    0.092083           AI\n"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "new_data = pd.read_csv(\"../../datasets/dataset1_inputs.csv\", delimiter=\"\\t\") \n",
    "\n",
    "# Tokenize and pad the new data\n",
    "X_new_seq = tokenizer.texts_to_sequences(new_data[\"Text\"])\n",
    "X_new = pad_sequences(X_new_seq, maxlen=100)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new).flatten()\n",
    "\n",
    "# Convert predictions to labels\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\"ID\": new_data[\"ID\"], \"Label\": labels, \"Prediction\": predictions})\n",
    "\n",
    "# Load the correct labels (ground truth)\n",
    "ground_truth = pd.read_csv(\"../../datasets/dataset1_outputs.csv\", delimiter=\"\\t\")  # Ensure it's tab-separated\n",
    "\n",
    "# Merge predictions with ground truth\n",
    "comparison_df = output_df.merge(ground_truth, on=\"ID\", suffixes=(\"_predicted\", \"_actual\"))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (comparison_df[\"Label_predicted\"] == comparison_df[\"Label_actual\"]).mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label_predicted\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)\n",
    "\n",
    "\n",
    "print(\"\\n-----------------------------------\\nAll Samples:\")\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These other combinations also provide good results\n",
    "### i.e., >= 0.6 for the professor data\n",
    "\n",
    "# Epochs: 10, Batch Size: 16, Learning Rate: 0.001\n",
    "# Epochs: 10, Batch Size: 16, Learning Rate: 0.0002\n",
    "# Epochs: 10, Batch Size: 32, Learning Rate: 0.0005\n",
    "# Epochs: 20, Batch Size: 16, Learning Rate: 0.0002\n",
    "# Epochs: 20, Batch Size: 64, Learning Rate: 0.0002"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
