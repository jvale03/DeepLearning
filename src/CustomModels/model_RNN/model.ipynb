{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recurrent_neural_net import RecurrentNeuralNetwork\n",
    "from tokenizer import  AdvancedTokenizer, RobustTokenizer, SimpleTokenizer\n",
    "from data import read_csv, read_csv_once\n",
    "from activation import SigmoidActivation, ReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, DropoutLayer, BatchNormalizationLayer, RNNLayer\n",
    "from losses import BinaryCrossEntropy   \n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Created model architecture\n",
      "Added layers to model\n",
      "\n",
      "Epoch 1/5\n",
      "Epoch 1/5 - loss: 0.8054 - accuracy: 0.4730 - val_loss: 0.6413 - val_accuracy: 0.7628\n",
      "\n",
      "Epoch 2/5\n",
      "Epoch 2/5 - loss: 0.6934 - accuracy: 0.5512 - val_loss: 0.7100 - val_accuracy: 0.4811\n",
      "\n",
      "Epoch 3/5\n",
      "Epoch 3/5 - loss: 0.6904 - accuracy: 0.5435 - val_loss: 0.6997 - val_accuracy: 0.4761\n",
      "\n",
      "Epoch 4/5\n",
      "Epoch 4/5 - loss: 0.6889 - accuracy: 0.5480 - val_loss: 0.6976 - val_accuracy: 0.4382\n",
      "\n",
      "Epoch 5/5\n",
      "Epoch 5/5 - loss: 0.6871 - accuracy: 0.5508 - val_loss: 0.6927 - val_accuracy: 0.5222\n",
      "Model trained\n",
      "Accuracy no dataset de teste: 0.5172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = SimpleTokenizer(num_words=10000, seed=GLOBAL_SEED)\n",
    "print(\"CSV tokenized!\")\n",
    "train_data, validation_data, test_data = read_csv(csv, tokenizer, seed=GLOBAL_SEED)\n",
    "# Creating a RNN model\n",
    "rnn = RecurrentNeuralNetwork(\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,\n",
    "    verbose=True,\n",
    "    seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "print('Created model architecture')\n",
    "n_features = train_data.X.shape[1]\n",
    "# Build RNN architecture\n",
    "rnn.add(EmbeddingLayer(vocab_size=10000, embedding_dim=8, input_shape=(n_features,)))\n",
    "rnn.add(RNNLayer(32, return_sequences=True, bptt_trunc=None))\n",
    "rnn.add(RNNLayer(16, return_sequences=False, bptt_trunc=None))\n",
    "rnn.add(BatchNormalizationLayer())\n",
    "rnn.add(ReLUActivation())\n",
    "rnn.add(DropoutLayer(dropout_rate=0.3))\n",
    "rnn.add(DenseLayer(8))\n",
    "rnn.add(ReLUActivation())\n",
    "rnn.add(DenseLayer(1))\n",
    "rnn.add(SigmoidActivation())\n",
    "print('Added layers to model')\n",
    "\n",
    "# Train the model\n",
    "rnn.fit(train_data, validation_data=validation_data, patience=5)\n",
    "print('Model trained')\n",
    "\n",
    "test_predictions = rnn.predict(test_data)\n",
    "test_score = rnn.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5125\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "2   Human    0.483094           AI\n",
      "5   Human    0.485387           AI\n",
      "6   Human    0.486265           AI\n",
      "9   Human    0.486478           AI\n",
      "10  Human    0.486469           AI\n",
      "11  Human    0.486256           AI\n",
      "12  Human    0.486482           AI\n",
      "13  Human    0.486482           AI\n",
      "16  Human    0.486175           AI\n",
      "18  Human    0.486179           AI\n",
      "22  Human    0.486481           AI\n",
      "24  Human    0.486473           AI\n",
      "28  Human    0.486480           AI\n",
      "33  Human    0.486509           AI\n",
      "34  Human    0.486483           AI\n",
      "35  Human    0.486482           AI\n",
      "37  Human    0.486480           AI\n",
      "38  Human    0.486465           AI\n",
      "39  Human    0.486482           AI\n",
      "40  Human    0.486468           AI\n",
      "42  Human    0.486513           AI\n",
      "43  Human    0.486089           AI\n",
      "44  Human    0.481505           AI\n",
      "45  Human    0.486451           AI\n",
      "51  Human    0.486483           AI\n",
      "53  Human    0.486466           AI\n",
      "55  Human    0.486483           AI\n",
      "57  Human    0.486482           AI\n",
      "59  Human    0.478796           AI\n",
      "61  Human    0.486470           AI\n",
      "63  Human    0.486483           AI\n",
      "65  Human    0.479582           AI\n",
      "67  Human    0.486427           AI\n",
      "69  Human    0.486480           AI\n",
      "71  Human    0.486483           AI\n",
      "73  Human    0.486230           AI\n",
      "75  Human    0.486509           AI\n",
      "77  Human    0.486483           AI\n",
      "79  Human    0.486464           AI\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_csv_once(new_file, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = rnn.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracys = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracys:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Mostrar amostras mal classificadas\n",
    "misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
