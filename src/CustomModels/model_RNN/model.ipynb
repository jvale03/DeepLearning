{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recurrent_neural_net import RecurrentNeuralNetwork\n",
    "from tokenizer import  AdvancedTokenizer, RobustTokenizer, SimpleTokenizer\n",
    "from data import read_csv, read_csv_once\n",
    "from activation import SigmoidActivation, ReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, DropoutLayer, BatchNormalizationLayer, RNNLayer\n",
    "from losses import BinaryCrossEntropy   \n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Created model architecture\n",
      "Added layers to model\n",
      "\n",
      "Epoch 1/5\n",
      "Epoch 1/5 - loss: 0.7033 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5206\n",
      "\n",
      "Epoch 2/5\n",
      "Epoch 2/5 - loss: 0.7056 - accuracy: 0.5020 - val_loss: 0.6927 - val_accuracy: 0.5058\n",
      "\n",
      "Epoch 3/5\n",
      "Epoch 3/5 - loss: 0.6922 - accuracy: 0.5278 - val_loss: 0.6901 - val_accuracy: 0.5437\n",
      "\n",
      "Epoch 4/5\n",
      "Epoch 4/5 - loss: 0.6871 - accuracy: 0.5474 - val_loss: 0.6848 - val_accuracy: 0.5931\n",
      "\n",
      "Epoch 5/5\n",
      "Epoch 5/5 - loss: 0.6775 - accuracy: 0.5791 - val_loss: 0.6705 - val_accuracy: 0.6705\n",
      "Model trained\n",
      "Accuracy no dataset de teste: 0.6831\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000 \n",
    "EMBEDDING_DIM = 100 \n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = SimpleTokenizer(num_words=10000, seed=GLOBAL_SEED)\n",
    "print(\"CSV tokenized!\")\n",
    "train_data, validation_data, test_data = read_csv(csv, tokenizer, seed=GLOBAL_SEED)\n",
    "# Creating a RNN model\n",
    "rnn = RecurrentNeuralNetwork(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    momentum=0.9,\n",
    "    verbose=True,\n",
    "    seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "print('Created model architecture')\n",
    "n_features = train_data.X.shape[1]\n",
    "# Build RNN architecture\n",
    "rnn.add(EmbeddingLayer(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, input_shape=(n_features,)))\n",
    "rnn.add(RNNLayer(128, return_sequences=False, bptt_trunc=None)) \n",
    "rnn.add(DropoutLayer(dropout_rate=0.5))\n",
    "rnn.add(DenseLayer(1))\n",
    "rnn.add(SigmoidActivation())\n",
    "print('Added layers to model')\n",
    "\n",
    "# Train the model\n",
    "rnn.fit(train_data, validation_data=validation_data, patience=5)\n",
    "print('Model trained')\n",
    "\n",
    "test_predictions = rnn.predict(test_data)\n",
    "test_score = rnn.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5750\n",
      "\n",
      "Predictions:\n",
      "    Label  Prediction Label_actual\n",
      "0   Human    0.456037        Human\n",
      "1   Human    0.479780        Human\n",
      "2   Human    0.496886           AI\n",
      "3   Human    0.462592        Human\n",
      "4   Human    0.462459        Human\n",
      "5   Human    0.468886           AI\n",
      "6   Human    0.463228           AI\n",
      "7   Human    0.496758        Human\n",
      "8   Human    0.482349        Human\n",
      "9   Human    0.457116           AI\n",
      "10  Human    0.449208           AI\n",
      "11  Human    0.452166           AI\n",
      "12     AI    0.542790           AI\n",
      "13  Human    0.447342           AI\n",
      "14  Human    0.486260        Human\n",
      "15  Human    0.447898        Human\n",
      "16     AI    0.524998           AI\n",
      "17  Human    0.463383        Human\n",
      "18  Human    0.448392           AI\n",
      "19  Human    0.446157        Human\n",
      "20  Human    0.471652        Human\n",
      "21  Human    0.451515        Human\n",
      "22  Human    0.478057           AI\n",
      "23  Human    0.466906        Human\n",
      "24  Human    0.463049           AI\n",
      "25  Human    0.462574        Human\n",
      "26  Human    0.461016        Human\n",
      "27  Human    0.468733        Human\n",
      "28  Human    0.484663           AI\n",
      "29     AI    0.524966        Human\n",
      "30  Human    0.477485        Human\n",
      "31  Human    0.490809        Human\n",
      "32  Human    0.464101        Human\n",
      "33  Human    0.486666           AI\n",
      "34  Human    0.491858           AI\n",
      "35  Human    0.471456           AI\n",
      "36  Human    0.449110        Human\n",
      "37     AI    0.503693           AI\n",
      "38     AI    0.504742           AI\n",
      "39  Human    0.470933           AI\n",
      "40  Human    0.499714           AI\n",
      "41  Human    0.487283        Human\n",
      "42  Human    0.454727           AI\n",
      "43  Human    0.449104           AI\n",
      "44  Human    0.458625           AI\n",
      "45  Human    0.468867           AI\n",
      "46  Human    0.454184        Human\n",
      "47  Human    0.471118        Human\n",
      "48     AI    0.528278        Human\n",
      "49  Human    0.488680        Human\n",
      "50     AI    0.533505        Human\n",
      "51  Human    0.465412           AI\n",
      "52  Human    0.497867        Human\n",
      "53     AI    0.517475           AI\n",
      "54  Human    0.493730        Human\n",
      "55     AI    0.540466           AI\n",
      "56  Human    0.456290        Human\n",
      "57  Human    0.458322           AI\n",
      "58  Human    0.459406        Human\n",
      "59  Human    0.486565           AI\n",
      "60  Human    0.456326        Human\n",
      "61  Human    0.478872           AI\n",
      "62  Human    0.463650        Human\n",
      "63  Human    0.466388           AI\n",
      "64  Human    0.464020        Human\n",
      "65  Human    0.479762           AI\n",
      "66  Human    0.444465        Human\n",
      "67  Human    0.475437           AI\n",
      "68  Human    0.461860        Human\n",
      "69     AI    0.538870           AI\n",
      "70  Human    0.460404        Human\n",
      "71     AI    0.562064           AI\n",
      "72  Human    0.470126        Human\n",
      "73  Human    0.465225           AI\n",
      "74  Human    0.470627        Human\n",
      "75     AI    0.525026           AI\n",
      "76     AI    0.511743        Human\n",
      "77  Human    0.481828           AI\n",
      "78  Human    0.465690        Human\n",
      "79  Human    0.459085           AI\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_csv_once(new_file, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = rnn.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracys = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracys:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(output_df)\n",
    "\n",
    "## Mostrar amostras mal classificadas\n",
    "#misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "#print(\"\\nMisclassified Samples:\")\n",
    "#print(misclassified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
