{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_neural_net import DeepNeuralNetwork\n",
    "from tokenizer import  AdvancedTokenizer, RobustTokenizer, SimpleTokenizer\n",
    "from data import read_csv, read_csv_once\n",
    "from activation import SigmoidActivation, ReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, DropoutLayer, BatchNormalizationLayer, GlobalAveragePooling1D\n",
    "from losses import BinaryCrossEntropy   \n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Epoch 1/10 - loss: 0.8770 - accuracy: 0.5248 - val_loss: 0.7790 - val_accuracy: 0.5189\n",
      "Epoch 2/10 - loss: 0.8557 - accuracy: 0.5230 - val_loss: 0.6946 - val_accuracy: 0.4514\n",
      "Epoch 3/10 - loss: 0.8805 - accuracy: 0.5121 - val_loss: 0.8067 - val_accuracy: 0.5189\n",
      "Epoch 4/10 - loss: 0.8680 - accuracy: 0.5192 - val_loss: 0.7357 - val_accuracy: 0.5189\n",
      "Epoch 5/10 - loss: 0.8549 - accuracy: 0.5073 - val_loss: 0.7348 - val_accuracy: 0.5189\n",
      "Epoch 6/10 - loss: 0.8428 - accuracy: 0.5199 - val_loss: 0.7340 - val_accuracy: 0.5189\n",
      "Epoch 7/10 - loss: 0.8491 - accuracy: 0.5232 - val_loss: 0.7497 - val_accuracy: 0.5189\n",
      "Epoch 8/10 - loss: 0.7495 - accuracy: 0.5073 - val_loss: 0.6942 - val_accuracy: 0.4811\n",
      "Epoch 9/10 - loss: 0.7046 - accuracy: 0.5115 - val_loss: 0.6931 - val_accuracy: 0.5206\n",
      "Epoch 10/10 - loss: 0.7097 - accuracy: 0.5025 - val_loss: 0.6938 - val_accuracy: 0.4811\n",
      "Accuracy no dataset de teste: 0.4844\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = AdvancedTokenizer(num_words=7500, seed=GLOBAL_SEED)\n",
    "print(\"CSV tokenized!\")\n",
    "\n",
    "train_data, validation_data, test_data = read_csv(csv, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "net = DeepNeuralNetwork(epochs=10, batch_size=16, learning_rate=0.001, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy, seed=GLOBAL_SEED)\n",
    "\n",
    "n_features = train_data.X.shape[1]\n",
    "\n",
    "net.add(EmbeddingLayer(vocab_size=7500, embedding_dim=100, input_shape=(n_features,)))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "\n",
    "\n",
    "net.add(DenseLayer(64, l2=0.001))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(32, l2=0.001))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(16, l2=0.001))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.3))\n",
    "\n",
    "net.add(DenseLayer(1))  \n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "net.fit(train_data,validation_data=validation_data, patience=20)\n",
    "\n",
    "test_predictions = net.predict(test_data)\n",
    "test_score = net.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4875\n",
      "\n",
      "Misclassified Samples:\n",
      "   Label  Prediction Label_actual\n",
      "0     AI    0.507357        Human\n",
      "1     AI    0.507357        Human\n",
      "3     AI    0.507357        Human\n",
      "4     AI    0.507357        Human\n",
      "7     AI    0.507357        Human\n",
      "8     AI    0.507357        Human\n",
      "14    AI    0.507357        Human\n",
      "15    AI    0.507357        Human\n",
      "17    AI    0.507357        Human\n",
      "19    AI    0.507357        Human\n",
      "20    AI    0.507357        Human\n",
      "21    AI    0.507357        Human\n",
      "23    AI    0.507357        Human\n",
      "25    AI    0.507357        Human\n",
      "26    AI    0.507357        Human\n",
      "27    AI    0.507357        Human\n",
      "29    AI    0.507357        Human\n",
      "30    AI    0.507357        Human\n",
      "31    AI    0.507357        Human\n",
      "32    AI    0.507357        Human\n",
      "36    AI    0.507357        Human\n",
      "41    AI    0.507357        Human\n",
      "46    AI    0.507357        Human\n",
      "47    AI    0.507357        Human\n",
      "48    AI    0.507357        Human\n",
      "49    AI    0.507357        Human\n",
      "50    AI    0.507357        Human\n",
      "52    AI    0.507357        Human\n",
      "54    AI    0.507357        Human\n",
      "56    AI    0.507357        Human\n",
      "58    AI    0.507357        Human\n",
      "60    AI    0.507357        Human\n",
      "62    AI    0.507357        Human\n",
      "64    AI    0.507357        Human\n",
      "66    AI    0.507357        Human\n",
      "68    AI    0.507357        Human\n",
      "70    AI    0.507357        Human\n",
      "72    AI    0.507357        Human\n",
      "74    AI    0.507357        Human\n",
      "76    AI    0.507357        Human\n",
      "78    AI    0.507357        Human\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_csv_once(new_file, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = net.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracys = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracys:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Mostrar amostras mal classificadas\n",
    "misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
