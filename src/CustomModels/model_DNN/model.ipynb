{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_neural_net import DeepNeuralNetwork\n",
    "from tokenizer import  AdvancedTokenizer, RobustTokenizer, SimpleTokenizer\n",
    "from data import read_csv, read_csv_once\n",
    "from activation import SigmoidActivation, ReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, DropoutLayer, BatchNormalizationLayer, GlobalAveragePooling1D\n",
    "from losses import BinaryCrossEntropy   \n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Epoch 1/50 - loss: 0.8608 - accuracy: 0.4995 - val_loss: 0.7046 - val_accuracy: 0.5025\n",
      "Epoch 2/50 - loss: 0.8259 - accuracy: 0.5104 - val_loss: 0.7111 - val_accuracy: 0.7183\n",
      "Epoch 3/50 - loss: 0.8031 - accuracy: 0.5287 - val_loss: 0.7023 - val_accuracy: 0.6672\n",
      "Epoch 4/50 - loss: 0.7997 - accuracy: 0.5386 - val_loss: 0.7138 - val_accuracy: 0.5848\n",
      "Epoch 5/50 - loss: 0.8130 - accuracy: 0.5129 - val_loss: 0.7149 - val_accuracy: 0.5124\n",
      "Epoch 6/50 - loss: 0.7890 - accuracy: 0.5198 - val_loss: 0.7203 - val_accuracy: 0.7249\n",
      "Epoch 7/50 - loss: 0.7875 - accuracy: 0.5198 - val_loss: 0.7333 - val_accuracy: 0.6903\n",
      "Epoch 8/50 - loss: 0.7888 - accuracy: 0.5355 - val_loss: 0.7353 - val_accuracy: 0.7512\n",
      "Epoch 9/50 - loss: 0.7716 - accuracy: 0.5381 - val_loss: 0.7206 - val_accuracy: 0.6672\n",
      "Epoch 10/50 - loss: 0.7517 - accuracy: 0.5603 - val_loss: 0.7084 - val_accuracy: 0.7479\n",
      "Epoch 11/50 - loss: 0.7895 - accuracy: 0.5202 - val_loss: 0.7183 - val_accuracy: 0.6936\n",
      "Epoch 12/50 - loss: 0.7372 - accuracy: 0.5608 - val_loss: 0.7222 - val_accuracy: 0.6886\n",
      "Epoch 13/50 - loss: 0.7329 - accuracy: 0.5712 - val_loss: 0.7634 - val_accuracy: 0.5980\n",
      "Epoch 14/50 - loss: 0.7726 - accuracy: 0.5317 - val_loss: 0.6999 - val_accuracy: 0.7875\n",
      "Epoch 15/50 - loss: 0.7519 - accuracy: 0.5512 - val_loss: 0.7661 - val_accuracy: 0.7990\n",
      "Epoch 16/50 - loss: 0.7302 - accuracy: 0.5596 - val_loss: 0.7404 - val_accuracy: 0.8320\n",
      "Epoch 17/50 - loss: 0.7633 - accuracy: 0.5441 - val_loss: 0.7668 - val_accuracy: 0.8072\n",
      "Epoch 18/50 - loss: 0.7717 - accuracy: 0.5314 - val_loss: 0.7072 - val_accuracy: 0.4860\n",
      "Epoch 19/50 - loss: 0.7776 - accuracy: 0.5206 - val_loss: 0.6997 - val_accuracy: 0.7249\n",
      "Epoch 20/50 - loss: 0.7391 - accuracy: 0.5624 - val_loss: 0.7140 - val_accuracy: 0.7661\n",
      "Epoch 21/50 - loss: 0.7411 - accuracy: 0.5581 - val_loss: 0.7054 - val_accuracy: 0.7628\n",
      "Epoch 22/50 - loss: 0.8034 - accuracy: 0.5039 - val_loss: 0.6983 - val_accuracy: 0.4860\n",
      "Epoch 23/50 - loss: 0.7642 - accuracy: 0.5279 - val_loss: 0.6979 - val_accuracy: 0.5799\n",
      "Epoch 24/50 - loss: 0.7718 - accuracy: 0.5283 - val_loss: 0.7025 - val_accuracy: 0.5387\n",
      "Epoch 25/50 - loss: 0.7927 - accuracy: 0.5092 - val_loss: 0.6939 - val_accuracy: 0.3394\n",
      "Epoch 26/50 - loss: 0.8003 - accuracy: 0.4971 - val_loss: 0.7218 - val_accuracy: 0.8336\n",
      "Epoch 27/50 - loss: 0.7586 - accuracy: 0.5321 - val_loss: 0.7119 - val_accuracy: 0.8303\n",
      "Epoch 28/50 - loss: 0.7483 - accuracy: 0.5360 - val_loss: 0.7105 - val_accuracy: 0.6820\n",
      "Epoch 29/50 - loss: 0.7310 - accuracy: 0.5417 - val_loss: 0.7098 - val_accuracy: 0.5552\n",
      "Epoch 30/50 - loss: 0.7413 - accuracy: 0.5447 - val_loss: 0.7208 - val_accuracy: 0.8435\n",
      "Epoch 31/50 - loss: 0.7510 - accuracy: 0.5315 - val_loss: 0.6969 - val_accuracy: 0.5157\n",
      "Epoch 32/50 - loss: 0.7599 - accuracy: 0.5298 - val_loss: 0.7053 - val_accuracy: 0.4811\n",
      "Epoch 33/50 - loss: 0.7791 - accuracy: 0.5135 - val_loss: 0.6976 - val_accuracy: 0.4811\n",
      "Epoch 34/50 - loss: 0.7577 - accuracy: 0.5294 - val_loss: 0.6998 - val_accuracy: 0.4827\n",
      "Epoch 35/50 - loss: 0.7538 - accuracy: 0.5225 - val_loss: 0.6930 - val_accuracy: 0.3410\n",
      "Epoch 36/50 - loss: 0.7405 - accuracy: 0.5134 - val_loss: 0.6936 - val_accuracy: 0.2685\n",
      "Epoch 37/50 - loss: 0.7349 - accuracy: 0.5223 - val_loss: 0.6973 - val_accuracy: 0.4185\n",
      "Epoch 38/50 - loss: 0.7262 - accuracy: 0.5338 - val_loss: 0.6947 - val_accuracy: 0.1730\n",
      "Epoch 39/50 - loss: 0.7263 - accuracy: 0.5223 - val_loss: 0.6926 - val_accuracy: 0.5189\n",
      "Epoch 40/50 - loss: 0.7251 - accuracy: 0.4995 - val_loss: 0.6927 - val_accuracy: 0.5189\n",
      "Epoch 41/50 - loss: 0.7260 - accuracy: 0.5015 - val_loss: 0.6930 - val_accuracy: 0.5189\n",
      "Epoch 42/50 - loss: 0.7193 - accuracy: 0.5235 - val_loss: 0.6930 - val_accuracy: 0.5189\n",
      "Epoch 43/50 - loss: 0.7337 - accuracy: 0.5137 - val_loss: 0.7059 - val_accuracy: 0.4827\n",
      "Epoch 44/50 - loss: 0.7743 - accuracy: 0.5129 - val_loss: 0.7121 - val_accuracy: 0.4811\n",
      "Epoch 45/50 - loss: 0.7431 - accuracy: 0.5315 - val_loss: 0.7007 - val_accuracy: 0.5157\n",
      "Epoch 46/50 - loss: 0.7504 - accuracy: 0.5180 - val_loss: 0.6931 - val_accuracy: 0.7183\n",
      "Epoch 47/50 - loss: 0.7623 - accuracy: 0.5019 - val_loss: 0.7082 - val_accuracy: 0.4827\n",
      "Epoch 48/50 - loss: 0.7578 - accuracy: 0.5055 - val_loss: 0.7143 - val_accuracy: 0.4926\n",
      "Epoch 49/50 - loss: 0.7594 - accuracy: 0.4994 - val_loss: 0.7071 - val_accuracy: 0.4909\n",
      "Epoch 50/50 - loss: 0.7288 - accuracy: 0.5326 - val_loss: 0.6968 - val_accuracy: 0.5964\n",
      "Accuracy no dataset de teste: 0.6141\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = AdvancedTokenizer(num_words=7500)\n",
    "print(\"CSV tokenized!\")\n",
    "\n",
    "train_data, validation_data, test_data = read_csv(csv, tokenizer)\n",
    "\n",
    "net = DeepNeuralNetwork(epochs=50, batch_size=16, learning_rate=0.001, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy)\n",
    "\n",
    "n_features = train_data.X.shape[1]\n",
    "\n",
    "net.add(EmbeddingLayer(vocab_size=7500, embedding_dim=100, input_shape=(n_features,)))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "\n",
    "\n",
    "net.add(DenseLayer(64, l2=0.001))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(32, l2=0.001))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(16, l2=0.001))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.3))\n",
    "\n",
    "net.add(DenseLayer(1))  \n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "net.fit(train_data,validation_data=validation_data, patience=20)\n",
    "\n",
    "test_predictions = net.predict(test_data)\n",
    "test_score = net.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5250\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "0      AI    0.501468        Human\n",
      "1      AI    0.519318        Human\n",
      "4      AI    0.505369        Human\n",
      "7      AI    0.521645        Human\n",
      "8      AI    0.588319        Human\n",
      "14     AI    0.511539        Human\n",
      "15     AI    0.506762        Human\n",
      "17     AI    0.512766        Human\n",
      "20     AI    0.518158        Human\n",
      "21     AI    0.518169        Human\n",
      "23     AI    0.513017        Human\n",
      "25     AI    0.503831        Human\n",
      "26     AI    0.501189        Human\n",
      "29     AI    0.507897        Human\n",
      "30     AI    0.509726        Human\n",
      "32     AI    0.521390        Human\n",
      "35  Human    0.493783           AI\n",
      "36     AI    0.518719        Human\n",
      "38  Human    0.495883           AI\n",
      "41     AI    0.513945        Human\n",
      "46     AI    0.508618        Human\n",
      "49     AI    0.505494        Human\n",
      "50     AI    0.509480        Human\n",
      "53  Human    0.491547           AI\n",
      "54     AI    0.507158        Human\n",
      "56     AI    0.521856        Human\n",
      "58     AI    0.516771        Human\n",
      "60     AI    0.512504        Human\n",
      "63  Human    0.491735           AI\n",
      "64     AI    0.526597        Human\n",
      "65  Human    0.492218           AI\n",
      "66     AI    0.521694        Human\n",
      "68     AI    0.514514        Human\n",
      "70     AI    0.508541        Human\n",
      "73  Human    0.492111           AI\n",
      "74     AI    0.518823        Human\n",
      "76     AI    0.510053        Human\n",
      "78     AI    0.521732        Human\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_csv_once(new_file, tokenizer)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = net.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracys = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracys:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Mostrar amostras mal classificadas\n",
    "misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4ano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
