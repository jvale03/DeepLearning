{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_neural_net import DeepNeuralNetwork\n",
    "from tokenizer import  AdvancedTokenizer, RobustTokenizer, SimpleTokenizer\n",
    "from data import read_csv, read_new_data\n",
    "from activation import SigmoidActivation, ReLUActivation, LeakyReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, DropoutLayer, BatchNormalizationLayer, GlobalAveragePooling1D, GRULayer, lstm_layer, FlattenLayer\n",
    "from losses import BinaryCrossEntropy\n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Epoch 1/26 - loss: 0.8965 - accuracy: 0.4142 - val_loss: 0.7079 - val_accuracy: 0.3556\n",
      "Epoch 2/26 - loss: 0.8817 - accuracy: 0.4280 - val_loss: 0.7252 - val_accuracy: 0.3679\n",
      "Epoch 3/26 - loss: 0.8685 - accuracy: 0.4289 - val_loss: 0.7225 - val_accuracy: 0.3778\n",
      "Epoch 4/26 - loss: 0.8550 - accuracy: 0.4335 - val_loss: 0.7200 - val_accuracy: 0.3827\n",
      "Epoch 5/26 - loss: 0.8424 - accuracy: 0.4363 - val_loss: 0.7178 - val_accuracy: 0.3901\n",
      "Epoch 6/26 - loss: 0.8309 - accuracy: 0.4372 - val_loss: 0.7167 - val_accuracy: 0.4000\n",
      "Epoch 7/26 - loss: 0.8192 - accuracy: 0.4430 - val_loss: 0.7141 - val_accuracy: 0.4148\n",
      "Epoch 8/26 - loss: 0.8084 - accuracy: 0.4470 - val_loss: 0.7125 - val_accuracy: 0.4222\n",
      "Epoch 9/26 - loss: 0.7983 - accuracy: 0.4507 - val_loss: 0.7112 - val_accuracy: 0.4346\n",
      "Epoch 10/26 - loss: 0.7887 - accuracy: 0.4531 - val_loss: 0.7098 - val_accuracy: 0.4667\n",
      "Epoch 11/26 - loss: 0.7794 - accuracy: 0.4559 - val_loss: 0.7089 - val_accuracy: 0.4716\n",
      "Epoch 12/26 - loss: 0.7707 - accuracy: 0.4586 - val_loss: 0.7082 - val_accuracy: 0.5012\n",
      "Epoch 13/26 - loss: 0.7628 - accuracy: 0.4648 - val_loss: 0.7075 - val_accuracy: 0.5259\n",
      "Epoch 14/26 - loss: 0.7549 - accuracy: 0.4715 - val_loss: 0.7069 - val_accuracy: 0.5383\n",
      "Epoch 15/26 - loss: 0.7474 - accuracy: 0.4746 - val_loss: 0.7066 - val_accuracy: 0.5531\n",
      "Epoch 16/26 - loss: 0.7404 - accuracy: 0.4862 - val_loss: 0.7066 - val_accuracy: 0.5753\n",
      "Epoch 17/26 - loss: 0.7332 - accuracy: 0.4942 - val_loss: 0.7064 - val_accuracy: 0.5951\n",
      "Epoch 18/26 - loss: 0.7267 - accuracy: 0.5025 - val_loss: 0.7066 - val_accuracy: 0.6099\n",
      "Epoch 19/26 - loss: 0.7213 - accuracy: 0.5147 - val_loss: 0.7080 - val_accuracy: 0.6272\n",
      "Epoch 20/26 - loss: 0.7155 - accuracy: 0.5251 - val_loss: 0.7082 - val_accuracy: 0.6370\n",
      "Epoch 21/26 - loss: 0.7096 - accuracy: 0.5322 - val_loss: 0.7074 - val_accuracy: 0.6519\n",
      "Epoch 22/26 - loss: 0.7043 - accuracy: 0.5374 - val_loss: 0.7089 - val_accuracy: 0.6494\n",
      "Epoch 23/26 - loss: 0.6992 - accuracy: 0.5499 - val_loss: 0.7086 - val_accuracy: 0.6667\n",
      "Epoch 24/26 - loss: 0.6942 - accuracy: 0.5564 - val_loss: 0.7099 - val_accuracy: 0.6716\n",
      "Epoch 25/26 - loss: 0.6897 - accuracy: 0.5594 - val_loss: 0.7101 - val_accuracy: 0.6815\n",
      "Epoch 26/26 - loss: 0.6854 - accuracy: 0.5646 - val_loss: 0.7112 - val_accuracy: 0.6790\n",
      "Accuracy no dataset de teste: 0.6700\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = AdvancedTokenizer(num_words=20000, seed=GLOBAL_SEED, min_freq=100, remove_stopwords=False)\n",
    "print(\"CSV tokenized!\")\n",
    "\n",
    "train_data, validation_data, test_data = read_csv(csv, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "\n",
    "net = DeepNeuralNetwork(epochs=26, batch_size=32, learning_rate=0.0001, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy, seed=GLOBAL_SEED)\n",
    "\n",
    "n_features = train_data.X.shape[1]\n",
    "\n",
    "net.add(EmbeddingLayer(vocab_size=20000, embedding_dim=100, input_shape=(n_features,)))\n",
    "net.add(GRULayer(n_units=8, return_sequences=True))\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "\n",
    "net.add(DenseLayer(4, l2=0.001))\n",
    "net.add(LeakyReLUActivation())\n",
    "net.add(BatchNormalizationLayer())\n",
    "net.add(DropoutLayer(0.6))\n",
    "\n",
    "net.add(DenseLayer(1))  \n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "net.fit(train_data,validation_data=validation_data, patience=20)\n",
    "\n",
    "test_predictions = net.predict(test_data)\n",
    "test_score = net.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4750\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "0      AI    0.506122        Human\n",
      "2   Human    0.260921           AI\n",
      "5   Human    0.332236           AI\n",
      "6   Human    0.435405           AI\n",
      "9   Human    0.474481           AI\n",
      "10  Human    0.339109           AI\n",
      "11  Human    0.407024           AI\n",
      "12  Human    0.241769           AI\n",
      "16  Human    0.386204           AI\n",
      "18  Human    0.302188           AI\n",
      "22  Human    0.474170           AI\n",
      "24  Human    0.357727           AI\n",
      "28  Human    0.486840           AI\n",
      "29     AI    0.527071        Human\n",
      "30     AI    0.607551        Human\n",
      "33  Human    0.340189           AI\n",
      "34  Human    0.346047           AI\n",
      "37  Human    0.305807           AI\n",
      "39  Human    0.323415           AI\n",
      "40  Human    0.471619           AI\n",
      "42  Human    0.348788           AI\n",
      "43  Human    0.411375           AI\n",
      "44  Human    0.444063           AI\n",
      "45  Human    0.312116           AI\n",
      "51  Human    0.390923           AI\n",
      "53  Human    0.451253           AI\n",
      "55  Human    0.328869           AI\n",
      "57  Human    0.363740           AI\n",
      "59  Human    0.427296           AI\n",
      "61  Human    0.454379           AI\n",
      "62     AI    0.618421        Human\n",
      "63  Human    0.466393           AI\n",
      "64     AI    0.536656        Human\n",
      "67  Human    0.306020           AI\n",
      "69  Human    0.327540           AI\n",
      "71  Human    0.300570           AI\n",
      "72     AI    0.504355        Human\n",
      "73  Human    0.322574           AI\n",
      "75  Human    0.358947           AI\n",
      "76     AI    0.544203        Human\n",
      "77  Human    0.309195           AI\n",
      "79  Human    0.461573           AI\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_new_data(new_file, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = net.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracys = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracys:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "\n",
    "\n",
    "# for _, row in output_df.iterrows():\n",
    "#     print(f\"{row['Label']} - {row['Prediction']}\")\n",
    "\n",
    "    \n",
    "misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4ano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
