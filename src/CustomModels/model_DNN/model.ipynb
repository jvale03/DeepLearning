{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_neural_net import DeepNeuralNetwork\n",
    "from tokenizer import SimpleTokenizer\n",
    "from data import read_csv\n",
    "from activation import SigmoidActivation, ReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, FlattenLayer, DropoutLayer, BatchNormalizationLayer, GlobalAveragePoolingLayer, GlobalAveragePooling1D\n",
    "from losses import BinaryCrossEntropy   \n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Epoch 1/25 - loss: 0.9840 - accuracy: 0.5071 - val_loss: 0.7117 - val_accuracy: 0.5288\n",
      "Epoch 2/25 - loss: 0.9914 - accuracy: 0.4968 - val_loss: 0.7028 - val_accuracy: 0.5585\n",
      "Epoch 3/25 - loss: 0.9804 - accuracy: 0.5059 - val_loss: 0.6989 - val_accuracy: 0.5914\n",
      "Epoch 4/25 - loss: 0.9536 - accuracy: 0.5105 - val_loss: 0.7002 - val_accuracy: 0.5371\n",
      "Epoch 5/25 - loss: 1.0449 - accuracy: 0.4971 - val_loss: 0.6939 - val_accuracy: 0.5997\n",
      "Epoch 6/25 - loss: 0.9285 - accuracy: 0.5182 - val_loss: 0.6998 - val_accuracy: 0.6112\n",
      "Epoch 7/25 - loss: 0.9879 - accuracy: 0.5024 - val_loss: 0.6988 - val_accuracy: 0.5750\n",
      "Epoch 8/25 - loss: 0.9878 - accuracy: 0.5057 - val_loss: 0.7041 - val_accuracy: 0.5881\n",
      "Epoch 9/25 - loss: 0.9537 - accuracy: 0.5097 - val_loss: 0.7022 - val_accuracy: 0.5997\n",
      "Epoch 10/25 - loss: 0.9423 - accuracy: 0.5099 - val_loss: 0.6971 - val_accuracy: 0.6079\n",
      "Epoch 11/25 - loss: 0.9085 - accuracy: 0.5105 - val_loss: 0.7046 - val_accuracy: 0.5733\n",
      "Epoch 12/25 - loss: 0.9293 - accuracy: 0.5185 - val_loss: 0.7021 - val_accuracy: 0.6260\n",
      "Epoch 13/25 - loss: 0.9126 - accuracy: 0.5134 - val_loss: 0.7037 - val_accuracy: 0.6507\n",
      "Epoch 14/25 - loss: 0.9395 - accuracy: 0.4950 - val_loss: 0.6995 - val_accuracy: 0.5848\n",
      "Epoch 15/25 - loss: 0.9270 - accuracy: 0.5117 - val_loss: 0.7001 - val_accuracy: 0.5519\n",
      "Epoch 16/25 - loss: 0.9362 - accuracy: 0.5059 - val_loss: 0.6993 - val_accuracy: 0.5502\n",
      "Epoch 17/25 - loss: 0.8996 - accuracy: 0.5105 - val_loss: 0.7074 - val_accuracy: 0.5321\n",
      "Epoch 18/25 - loss: 0.9041 - accuracy: 0.5146 - val_loss: 0.7031 - val_accuracy: 0.5437\n",
      "Epoch 19/25 - loss: 0.9052 - accuracy: 0.5073 - val_loss: 0.6945 - val_accuracy: 0.5288\n",
      "Epoch 20/25 - loss: 0.8970 - accuracy: 0.4988 - val_loss: 0.7019 - val_accuracy: 0.5288\n",
      "Epoch 21/25 - loss: 0.9120 - accuracy: 0.4966 - val_loss: 0.7047 - val_accuracy: 0.5288\n",
      "Epoch 22/25 - loss: 0.9060 - accuracy: 0.5080 - val_loss: 0.6972 - val_accuracy: 0.5288\n",
      "Epoch 23/25 - loss: 0.9162 - accuracy: 0.5085 - val_loss: 0.6989 - val_accuracy: 0.5288\n",
      "Epoch 24/25 - loss: 0.9286 - accuracy: 0.4990 - val_loss: 0.7006 - val_accuracy: 0.5288\n",
      "\n",
      "Early stopping ativado na epoch 25. Melhor val_loss: 0.6939\n",
      "Accuracy no dataset de teste: 0.5189\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = SimpleTokenizer(num_words=10000)\n",
    "print(\"CSV tokenized!\")\n",
    "\n",
    "train_data, validation_data, test_data = read_csv(file, tokenizer)\n",
    "\n",
    "net = DeepNeuralNetwork(epochs=25, batch_size=32, learning_rate=0.0005, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy)\n",
    "\n",
    "n_features = train_data.X.shape[1]\n",
    "\n",
    "net.add(EmbeddingLayer(vocab_size=10000, embedding_dim=128, input_shape=(n_features,)))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "\n",
    "\n",
    "net.add(DenseLayer(64, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(32, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(16, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "\n",
    "net.add(DenseLayer(8, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "\n",
    "net.add(DenseLayer(1))  \n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "net.fit(train_data,validation_data=validation_data, patience=20)\n",
    "\n",
    "test_predictions = net.predict(test_data)\n",
    "test_score = net.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    opt = input(\"Queres guardar? [y/n]\")\n",
    "    if opt == \"y\":\n",
    "        net.save(\"../../../models/modelo_dnn.pkl\")\n",
    "        break\n",
    "    elif opt == \"n\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(texts)  \u001b[38;5;66;03m# Ensure maxlen is consistent with your training data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert predictions to labels based on threshold\u001b[39;00m\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions]\n",
      "File \u001b[0;32m~/Documents/Universidade/4ºano/2semestre/AP/DeepLearning/src/CustomModels/model_DNN/deep_neural_net.py:113\u001b[0m, in \u001b[0;36mDeepNeuralNetwork.predict\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'X'"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "new_data = pd.read_csv(\"/Users/jvale03/Documents/Universidade/4ºano/2semestre/AP/DeepLearning/datasets/validation_dataset.csv\", delimiter=\";\")\n",
    "\n",
    "if new_data.shape[1] != 2:\n",
    "    raise ValueError(\"O dataset deve ter exatamente duas colunas: uma independente e uma dependente.\")\n",
    "\n",
    "texts = new_data.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_data[\"Label\"] = new_data[\"Label\"].map(category_mapping)\n",
    "new_data = new_data[[\"Text\", \"Label\"]]\n",
    "\n",
    "labels = new_data.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)  # Ensure maxlen is consistent with your training data\n",
    "\n",
    "# Make predictions\n",
    "predictions = net.predict(sequences)\n",
    "\n",
    "# Convert predictions to labels based on threshold\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame with predictions\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "# Load the ground truth labels (from the same dataset)\n",
    "# Since the labels are in the 'Label' column, we'll compare them with predictions.\n",
    "ground_truth = new_data[\"Label\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Merge predictions with ground truth for comparison\n",
    "comparison_df = output_df.copy()\n",
    "comparison_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4ano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
