{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_neural_net import DeepNeuralNetwork\n",
    "from tokenizer import SimpleTokenizer\n",
    "from data import read_csv, read_csv_once\n",
    "from activation import SigmoidActivation, ReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, FlattenLayer, DropoutLayer, BatchNormalizationLayer, GlobalAveragePoolingLayer, GlobalAveragePooling1D\n",
    "from losses import BinaryCrossEntropy   \n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Epoch 1/5 - loss: 0.9085 - accuracy: 0.4959 - val_loss: 0.6913 - val_accuracy: 0.5404\n",
      "Epoch 2/5 - loss: 0.9312 - accuracy: 0.4927 - val_loss: 0.6929 - val_accuracy: 0.6293\n",
      "Epoch 3/5 - loss: 0.9028 - accuracy: 0.4948 - val_loss: 0.6963 - val_accuracy: 0.5914\n",
      "Epoch 4/5 - loss: 0.8869 - accuracy: 0.5092 - val_loss: 0.6920 - val_accuracy: 0.5338\n",
      "Epoch 5/5 - loss: 0.9097 - accuracy: 0.5030 - val_loss: 0.6949 - val_accuracy: 0.4646\n",
      "Accuracy no dataset de teste: 0.4499\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = SimpleTokenizer(num_words=10000)\n",
    "print(\"CSV tokenized!\")\n",
    "\n",
    "train_data, validation_data, test_data = read_csv(file, tokenizer)\n",
    "\n",
    "net = DeepNeuralNetwork(epochs=5, batch_size=32, learning_rate=0.0005, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy)\n",
    "\n",
    "n_features = train_data.X.shape[1]\n",
    "\n",
    "net.add(EmbeddingLayer(vocab_size=10000, embedding_dim=128, input_shape=(n_features,)))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "\n",
    "\n",
    "net.add(DenseLayer(64, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(32, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(BatchNormalizationLayer())\n",
    "\n",
    "net.add(DenseLayer(16, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "\n",
    "net.add(DenseLayer(8, l2=0.003))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.5))\n",
    "\n",
    "net.add(DenseLayer(1))  \n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "net.fit(train_data,validation_data=validation_data, patience=20)\n",
    "\n",
    "test_predictions = net.predict(test_data)\n",
    "test_score = net.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    opt = input(\"Queres guardar? [y/n]\")\n",
    "    if opt == \"y\":\n",
    "        net.save(\"../../../models/modelo_dnn.pkl\")\n",
    "        break\n",
    "    elif opt == \"n\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4750\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "0      AI    0.510056        Human\n",
      "1      AI    0.513711        Human\n",
      "2   Human    0.476828           AI\n",
      "3      AI    0.518418        Human\n",
      "4      AI    0.527135        Human\n",
      "7      AI    0.531038        Human\n",
      "8      AI    0.524576        Human\n",
      "14     AI    0.527318        Human\n",
      "15     AI    0.543836        Human\n",
      "17     AI    0.524992        Human\n",
      "18  Human    0.494940           AI\n",
      "19     AI    0.514036        Human\n",
      "20     AI    0.535467        Human\n",
      "21     AI    0.534872        Human\n",
      "23     AI    0.505499        Human\n",
      "25     AI    0.518747        Human\n",
      "26     AI    0.543864        Human\n",
      "27     AI    0.579276        Human\n",
      "29     AI    0.516758        Human\n",
      "30     AI    0.521080        Human\n",
      "31     AI    0.547332        Human\n",
      "32     AI    0.526132        Human\n",
      "36     AI    0.523975        Human\n",
      "41     AI    0.521060        Human\n",
      "44  Human    0.498297           AI\n",
      "47     AI    0.511771        Human\n",
      "48     AI    0.508446        Human\n",
      "49     AI    0.515164        Human\n",
      "50     AI    0.526243        Human\n",
      "52     AI    0.550352        Human\n",
      "54     AI    0.528776        Human\n",
      "56     AI    0.527464        Human\n",
      "58     AI    0.516553        Human\n",
      "60     AI    0.502185        Human\n",
      "62     AI    0.510237        Human\n",
      "64     AI    0.502421        Human\n",
      "66     AI    0.515245        Human\n",
      "68     AI    0.530293        Human\n",
      "70     AI    0.539876        Human\n",
      "72     AI    0.503592        Human\n",
      "74     AI    0.518663        Human\n",
      "78     AI    0.553232        Human\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_csv_once(new_file, tokenizer)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = net.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracy = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "# Mostrar amostras mal classificadas\n",
    "misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cenas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
