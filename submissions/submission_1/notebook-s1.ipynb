{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src/CustomModels/model_DNN\")\n",
    "from deep_neural_net import DeepNeuralNetwork\n",
    "from tokenizer import  AdvancedTokenizer, RobustTokenizer, SimpleTokenizer\n",
    "from data import read_csv, read_new_data\n",
    "from activation import SigmoidActivation, ReLUActivation, LeakyReLUActivation\n",
    "from layers import DenseLayer, EmbeddingLayer, DropoutLayer, BatchNormalizationLayer, GlobalAveragePooling1D, GRULayer, FlattenLayer\n",
    "from losses import BinaryCrossEntropy\n",
    "from metrics import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '../../datasets/final_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing csv...\n",
      "CSV tokenized!\n",
      "Epoch 1/5 - loss: 0.8965 - accuracy: 0.4142 - val_loss: 0.7079 - val_accuracy: 0.3556\n",
      "Epoch 2/5 - loss: 0.8817 - accuracy: 0.4280 - val_loss: 0.7252 - val_accuracy: 0.3679\n",
      "Epoch 3/5 - loss: 0.8685 - accuracy: 0.4289 - val_loss: 0.7225 - val_accuracy: 0.3778\n",
      "Epoch 4/5 - loss: 0.8550 - accuracy: 0.4335 - val_loss: 0.7200 - val_accuracy: 0.3827\n",
      "Epoch 5/5 - loss: 0.8424 - accuracy: 0.4363 - val_loss: 0.7178 - val_accuracy: 0.3901\n",
      "Accuracy no dataset de teste: 0.3719\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing csv...\")\n",
    "tokenizer = AdvancedTokenizer(num_words=20000, seed=GLOBAL_SEED, min_freq=100, remove_stopwords=False)\n",
    "print(\"CSV tokenized!\")\n",
    "\n",
    "train_data, validation_data, test_data = read_csv(csv, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "\n",
    "net = DeepNeuralNetwork(epochs=26, batch_size=32, learning_rate=0.0001, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy, seed=GLOBAL_SEED)\n",
    "\n",
    "n_features = train_data.X.shape[1]\n",
    "\n",
    "net.add(EmbeddingLayer(vocab_size=20000, embedding_dim=100, input_shape=(n_features,)))\n",
    "net.add(GRULayer(n_units=8, return_sequences=True))\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "\n",
    "net.add(DenseLayer(4, l2=0.001))\n",
    "net.add(LeakyReLUActivation())\n",
    "net.add(BatchNormalizationLayer())\n",
    "net.add(DropoutLayer(0.6))\n",
    "\n",
    "net.add(DenseLayer(1))  \n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "net.fit(train_data,validation_data=validation_data, patience=20)\n",
    "\n",
    "test_predictions = net.predict(test_data)\n",
    "test_score = net.score(test_data, test_predictions)\n",
    "print(f\"Accuracy no dataset de teste: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5750\n",
      "\n",
      "Misclassified Samples:\n",
      "    Label  Prediction Label_actual\n",
      "1      AI    0.581674        Human\n",
      "2   Human    0.437257           AI\n",
      "3      AI    0.559224        Human\n",
      "6   Human    0.359242           AI\n",
      "8      AI    0.533037        Human\n",
      "10  Human    0.413121           AI\n",
      "13  Human    0.472326           AI\n",
      "14     AI    0.595648        Human\n",
      "15     AI    0.557086        Human\n",
      "16  Human    0.461640           AI\n",
      "21     AI    0.683000        Human\n",
      "22  Human    0.316102           AI\n",
      "29     AI    0.642195        Human\n",
      "31     AI    0.556055        Human\n",
      "33  Human    0.457123           AI\n",
      "36     AI    0.584414        Human\n",
      "37  Human    0.441942           AI\n",
      "41     AI    0.528720        Human\n",
      "51  Human    0.401298           AI\n",
      "53  Human    0.229541           AI\n",
      "55  Human    0.382901           AI\n",
      "57  Human    0.486281           AI\n",
      "58     AI    0.645560        Human\n",
      "60     AI    0.602299        Human\n",
      "62     AI    0.635695        Human\n",
      "67  Human    0.436916           AI\n",
      "68     AI    0.517660        Human\n",
      "70     AI    0.648701        Human\n",
      "71  Human    0.419764           AI\n",
      "72     AI    0.630746        Human\n",
      "73  Human    0.451937           AI\n",
      "74     AI    0.548728        Human\n",
      "76     AI    0.710684        Human\n",
      "78     AI    0.516505        Human\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"../../datasets/validation_dataset.csv\", sep=\";\")\n",
    "\n",
    "new_file = pd.DataFrame()\n",
    "# Mapear labels\n",
    "category_mapping = {\"Human\": 0, \"AI\": 1, \"student\": 0}\n",
    "new_file[\"Text\"] = file[\"Text\"]\n",
    "new_file[\"Label\"] = file[\"Label\"].map(category_mapping)\n",
    "\n",
    "# Processar os dados para a rede\n",
    "new_data = read_new_data(new_file, tokenizer, seed=GLOBAL_SEED)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = net.predict(new_data)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Converter previsões em rótulos\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Criar DataFrame com previsões\n",
    "output_df = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Carregar os rótulos reais e convertê-los para strings\n",
    "ground_truth = file[\"Label\"]\n",
    "\n",
    "# Calcular precisão\n",
    "accuracys = (output_df[\"Label\"] == ground_truth).mean()\n",
    "\n",
    "# Imprimir precisão\n",
    "print(f\"Accuracy: {accuracys:.4f}\")\n",
    "\n",
    "# Comparar previsões com rótulos reais\n",
    "output_df[\"Label_actual\"] = ground_truth\n",
    "\n",
    "\n",
    "\n",
    "# for _, row in output_df.iterrows():\n",
    "#     print(f\"{row['Label']} - {row['Prediction']}\")\n",
    "\n",
    "    \n",
    "misclassified = output_df[output_df[\"Label\"] != output_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4ano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
