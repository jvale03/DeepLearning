{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load spaCy model for better topic detection\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    print(\"Loaded spaCy model successfully\")\n",
    "except:\n",
    "    print(\"Downloading spaCy model (this may take a moment)...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_md\"])\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Define science/health/nutrition related terms\n",
    "TOPIC_KEYWORDS = {\n",
    "    'science': [\n",
    "        'science', 'research', 'experiment', 'theory', 'scientific', 'scientist', 'study', \n",
    "        'laboratory', 'evidence', 'hypothesis', 'analysis', 'discovery', 'biology', 'physics', \n",
    "        'chemistry', 'astronomy', 'geology', 'technology', 'innovation', 'engineering'\n",
    "    ],\n",
    "    'health': [\n",
    "        'health', 'medical', 'medicine', 'disease', 'doctor', 'patient', 'treatment', 'cure', \n",
    "        'symptoms', 'diagnosis', 'therapy', 'wellness', 'illness', 'healthcare', 'hospital', \n",
    "        'clinic', 'recovery', 'prevention', 'condition', 'immune', 'surgery', 'mental health',\n",
    "        'physical', 'wellbeing', 'disease', 'cancer', 'heart', 'brain', 'lungs'\n",
    "    ],\n",
    "    'nutrition': [\n",
    "        'nutrition', 'diet', 'food', 'healthy eating', 'nutrient', 'vitamin', 'mineral', \n",
    "        'protein', 'carbohydrate', 'fat', 'calorie', 'metabolism', 'digestion', 'meal', \n",
    "        'vegetable', 'fruit', 'meat', 'dairy', 'weight', 'obesity', 'sugar', 'organic', \n",
    "        'supplement', 'fiber', 'hydration', 'fasting', 'macronutrient'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten the keywords list for easier checking\n",
    "ALL_KEYWORDS = set()\n",
    "for category_keywords in TOPIC_KEYWORDS.values():\n",
    "    ALL_KEYWORDS.update(category_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace newlines and multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters except punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:\\'\"-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count words in text\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def is_relevant_topic(text, title):\n",
    "    \"\"\"\n",
    "    Check if the text is relevant to science, health, or nutrition\n",
    "    using both keyword matching and spaCy similarity\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Clean and lowercase the combined text\n",
    "    combined_text = (text + \" \" + title).lower()\n",
    "    \n",
    "    # Direct keyword matching\n",
    "    for keyword in ALL_KEYWORDS:\n",
    "        if keyword.lower() in combined_text:\n",
    "            return True\n",
    "    \n",
    "    # Use spaCy for semantic similarity\n",
    "    doc = nlp(combined_text)\n",
    "    \n",
    "    # Check similarity with topic keywords\n",
    "    for category, keywords in TOPIC_KEYWORDS.items():\n",
    "        for keyword in keywords:\n",
    "            keyword_doc = nlp(keyword)\n",
    "            # Check if any token in the document is similar to the keyword\n",
    "            for token in doc:\n",
    "                if token.vector_norm and keyword_doc[0].vector_norm:  # Check if vectors exist\n",
    "                    similarity = token.similarity(keyword_doc[0])\n",
    "                    if similarity > 0.75:  # Threshold for similarity\n",
    "                        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_arxiv_dataset(input_file, output_file, min_words=100, max_words=120, max_entries=1000):\n",
    "    \"\"\"\n",
    "    Filter arXiv abstracts dataset to get entries related to science, health, and nutrition\n",
    "    with word count between min_words and max_words, taking the first max_entries that match.\n",
    "    \"\"\"\n",
    "    # Read the input file and collect valid entries\n",
    "    valid_entries = []\n",
    "    line_count = 0\n",
    "    \n",
    "    # First pass: count total lines for progress bar\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            line_count += 1\n",
    "    \n",
    "    print(f\"Processing {line_count} entries from {input_file}...\")\n",
    "    \n",
    "    # Second pass: process entries\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for line in tqdm(f, total=line_count, desc=\"Filtering entries\"):\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                \n",
    "                # Get abstract and title\n",
    "                abstract = clean_text(entry.get('abstract', ''))\n",
    "                title = clean_text(entry.get('title', ''))\n",
    "                \n",
    "                # Check word count\n",
    "                word_count = count_words(abstract)\n",
    "                \n",
    "                # Check if the entry meets our criteria\n",
    "                if (min_words <= word_count <= max_words and \n",
    "                    is_relevant_topic(abstract, title)):\n",
    "                    \n",
    "                    # Create a new entry with the fields we want to keep\n",
    "                    filtered_entry = {\n",
    "                        'id': entry.get('id', ''),\n",
    "                        'title': title,\n",
    "                        'abstract': abstract,\n",
    "                        'authors': entry.get('authors', ''),\n",
    "                        'categories': entry.get('categories', []),\n",
    "                        'word_count': word_count\n",
    "                    }\n",
    "                    \n",
    "                    # Write directly to output file\n",
    "                    out_f.write(json.dumps(filtered_entry) + '\\n')\n",
    "                    \n",
    "                    valid_entries.append(filtered_entry)\n",
    "                    \n",
    "                    # Print progress periodically\n",
    "                    if len(valid_entries) % 100 == 0:\n",
    "                        print(f\"Found {len(valid_entries)} valid entries so far\")\n",
    "                    \n",
    "                    # Stop if we've reached the desired number of entries\n",
    "                    if len(valid_entries) >= max_entries:\n",
    "                        print(f\"Reached {max_entries} entries, stopping.\")\n",
    "                        break\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Found and saved {len(valid_entries)} entries matching criteria to {output_file}\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    word_counts = [entry['word_count'] for entry in valid_entries]\n",
    "    if word_counts:\n",
    "        avg_word_count = sum(word_counts) / len(word_counts)\n",
    "        print(f\"Average word count: {avg_word_count:.2f}\")\n",
    "        print(f\"Min word count: {min(word_counts)}\")\n",
    "        print(f\"Max word count: {max(word_counts)}\")\n",
    "    \n",
    "    return valid_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "input_file = \"arxiv-abstracts.jsonl\"  # Replace with your input file path\n",
    "output_file = \"filtered_arxiv_dataset.jsonl\"  # Replace with desired output file path\n",
    "min_words = 100  # Minimum word count\n",
    "max_words = 120  # Maximum word count\n",
    "max_entries = 1000  # Maximum number of entries to collect\n",
    "\n",
    "def run_filter():\n",
    "    \"\"\"Run the filtering process with the configured parameters\"\"\"\n",
    "    return filter_arxiv_dataset(\n",
    "        input_file=input_file,\n",
    "        output_file=output_file,\n",
    "        min_words=min_words,\n",
    "        max_words=max_words,\n",
    "        max_entries=max_entries\n",
    "    )\n",
    "\n",
    "\n",
    "# To analyze word count distribution\n",
    "def analyze_word_counts(jsonl_file):\n",
    "    \"\"\"Analyze word count distribution in the filtered dataset\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    word_counts = []\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            word_counts.append(entry['word_count'])\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame({'word_count': word_counts})\n",
    "    \n",
    "    # Display statistics\n",
    "    stats = df.describe()\n",
    "    print(\"Word Count Statistics:\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(word_counts, bins=20, alpha=0.7)\n",
    "    plt.title('Word Count Distribution')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to convert JSONL to CSV\n",
    "def convert_to_csv(jsonl_file, csv_file):\n",
    "    \"\"\"Convert a JSONL file to CSV format\"\"\"\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read JSONL into a list of dictionaries\n",
    "    data = []\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Removing all columns except abstract and word_count\n",
    "    df = df[['abstract', 'word_count']]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(csv_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "    print(f\"Converted {len(data)} entries to CSV format\")\n",
    "    print(f\"Saved to {csv_file}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Run the filter\n",
    "filtered_data = run_filter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word counts\n",
    "# word_count_data = analyze_word_counts(output_file)\n",
    "\n",
    "# Convert to CSV\n",
    "csv_df = convert_to_csv(output_file, \"filtered_abstracts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
