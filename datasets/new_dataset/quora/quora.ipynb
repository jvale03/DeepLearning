{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spaCy model successfully\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model for better topic detection\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    print(\"Loaded spaCy model successfully\")\n",
    "except:\n",
    "    print(\"Downloading spaCy model (this may take a moment)...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_md\"])\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Define science/health/nutrition related terms\n",
    "TOPIC_KEYWORDS = {\n",
    "    'science': [\n",
    "        'science', 'research', 'experiment', 'theory', 'scientific', 'scientist', 'study', \n",
    "        'laboratory', 'evidence', 'hypothesis', 'analysis', 'discovery', 'biology', 'physics', \n",
    "        'chemistry', 'astronomy', 'geology', 'technology', 'innovation', 'engineering'\n",
    "    ],\n",
    "    'health': [\n",
    "        'health', 'medical', 'medicine', 'disease', 'doctor', 'patient', 'treatment', 'cure', \n",
    "        'symptoms', 'diagnosis', 'therapy', 'wellness', 'illness', 'healthcare', 'hospital', \n",
    "        'clinic', 'recovery', 'prevention', 'condition', 'immune', 'surgery', 'mental health',\n",
    "        'physical', 'wellbeing', 'disease', 'cancer', 'heart', 'brain', 'lungs'\n",
    "    ],\n",
    "    'nutrition': [\n",
    "        'nutrition', 'diet', 'food', 'healthy eating', 'nutrient', 'vitamin', 'mineral', \n",
    "        'protein', 'carbohydrate', 'fat', 'calorie', 'metabolism', 'digestion', 'meal', \n",
    "        'vegetable', 'fruit', 'meat', 'dairy', 'weight', 'obesity', 'sugar', 'organic', \n",
    "        'supplement', 'fiber', 'hydration', 'fasting', 'macronutrient'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten the keywords list for easier checking\n",
    "ALL_KEYWORDS = set()\n",
    "for category_keywords in TOPIC_KEYWORDS.values():\n",
    "    ALL_KEYWORDS.update(category_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace newlines and multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters except punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:\\'\"-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count words in text\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def is_relevant_topic(text, question):\n",
    "    \"\"\"\n",
    "    Check if the text is relevant to science, health, or nutrition\n",
    "    using both keyword matching and spaCy similarity\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Clean and lowercase the combined text\n",
    "    combined_text = (text + \" \" + question).lower()\n",
    "    \n",
    "    # Direct keyword matching\n",
    "    for keyword in ALL_KEYWORDS:\n",
    "        if keyword.lower() in combined_text:\n",
    "            return True\n",
    "    \n",
    "    # Use spaCy for semantic similarity\n",
    "    doc = nlp(combined_text)\n",
    "    \n",
    "    # Check similarity with topic keywords\n",
    "    for category, keywords in TOPIC_KEYWORDS.items():\n",
    "        for keyword in keywords:\n",
    "            keyword_doc = nlp(keyword)\n",
    "            # Check if any token in the document is similar to the keyword\n",
    "            for token in doc:\n",
    "                if token.vector_norm and keyword_doc[0].vector_norm:  # Check if vectors exist\n",
    "                    similarity = token.similarity(keyword_doc[0])\n",
    "                    if similarity > 0.75:  # Threshold for similarity\n",
    "                        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_dataset(input_file, output_file, min_words=100, max_words=120, max_entries=1000):\n",
    "    \"\"\"\n",
    "    Filter JSONL dataset to get entries related to science, health, and nutrition\n",
    "    with word count between min_words and max_words\n",
    "    \"\"\"\n",
    "    # Read the input file and collect valid entries\n",
    "    valid_entries = []\n",
    "    line_count = 0\n",
    "    \n",
    "    # First pass: count total lines for progress bar\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            line_count += 1\n",
    "    \n",
    "    print(f\"Processing {line_count} entries from {input_file}...\")\n",
    "    \n",
    "    # Second pass: process entries\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=line_count, desc=\"Filtering entries\"):\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                \n",
    "                # Clean the text\n",
    "                answer = clean_text(entry.get('answer', ''))\n",
    "                question = clean_text(entry.get('question', ''))\n",
    "                \n",
    "                # Check word count\n",
    "                word_count = count_words(answer)\n",
    "                \n",
    "                # Check if the entry meets our criteria\n",
    "                if (min_words <= word_count <= max_words and \n",
    "                    is_relevant_topic(answer, question)):\n",
    "                    \n",
    "                    # Add the entry along with topic and word count info\n",
    "                    valid_entries.append({\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'word_count': word_count\n",
    "                    })\n",
    "                    \n",
    "                    # Print progress periodically\n",
    "                    if len(valid_entries) % 100 == 0:\n",
    "                        print(f\"Found {len(valid_entries)} valid entries so far\")\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Found {len(valid_entries)} entries matching criteria\")\n",
    "    \n",
    "    # If we have more entries than needed, randomly sample\n",
    "    if len(valid_entries) > max_entries:\n",
    "        print(f\"Randomly sampling {max_entries} entries from {len(valid_entries)} valid entries\")\n",
    "        valid_entries = random.sample(valid_entries, max_entries)\n",
    "    \n",
    "    # Write the filtered entries to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for entry in valid_entries:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(valid_entries)} entries to {output_file}\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    word_counts = [entry['word_count'] for entry in valid_entries]\n",
    "    if word_counts:\n",
    "        avg_word_count = sum(word_counts) / len(word_counts)\n",
    "        print(f\"Average word count: {avg_word_count:.2f}\")\n",
    "        print(f\"Min word count: {min(word_counts)}\")\n",
    "        print(f\"Max word count: {max(word_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert JSONL to CSV\n",
    "def convert_to_csv(jsonl_file, csv_file):\n",
    "    \"\"\"Convert a JSONL file to CSV format\"\"\"\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read JSONL into a list of dictionaries\n",
    "    data = []\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Removing the question column\n",
    "    df = df.drop(columns=['question'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(csv_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "    print(f\"Converted {len(data)} entries to CSV format\")\n",
    "    print(f\"Saved to {csv_file}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 56402 entries from Quora.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:   5%|▍         | 2619/56402 [00:34<08:13, 108.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:   8%|▊         | 4561/56402 [01:02<09:28, 91.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  12%|█▏        | 6842/56402 [01:24<07:35, 108.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  16%|█▌        | 9036/56402 [01:59<04:17, 184.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  19%|█▉        | 10889/56402 [02:22<10:18, 73.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  23%|██▎       | 12887/56402 [02:57<07:54, 91.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  26%|██▌       | 14630/56402 [03:30<16:20, 42.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  29%|██▉       | 16570/56402 [04:04<09:08, 72.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  33%|███▎      | 18729/56402 [04:44<09:45, 64.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  37%|███▋      | 20741/56402 [05:13<08:50, 67.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  40%|████      | 22784/56402 [05:40<05:18, 105.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1100 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  44%|████▍     | 24760/56402 [06:15<04:00, 131.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  47%|████▋     | 26527/56402 [06:52<08:31, 58.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1300 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  50%|█████     | 28280/56402 [07:15<03:46, 124.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1400 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  54%|█████▎    | 30259/56402 [07:43<02:59, 145.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  57%|█████▋    | 32396/56402 [08:17<05:27, 73.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  61%|██████    | 34330/56402 [08:46<03:38, 101.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1700 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  65%|██████▍   | 36439/56402 [09:19<02:11, 152.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  68%|██████▊   | 38394/56402 [09:41<02:51, 104.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1900 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  72%|███████▏  | 40418/56402 [10:13<05:46, 46.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  76%|███████▌  | 42834/56402 [10:52<05:17, 42.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2100 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  80%|███████▉  | 44972/56402 [11:23<02:36, 73.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2200 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  83%|████████▎ | 47005/56402 [12:00<02:30, 62.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2300 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  87%|████████▋ | 48911/56402 [12:27<00:38, 192.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  90%|█████████ | 50934/56402 [12:57<01:06, 82.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  94%|█████████▎| 52738/56402 [13:25<00:20, 175.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2600 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries:  98%|█████████▊| 55097/56402 [14:09<00:16, 79.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2700 valid entries so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering entries: 100%|██████████| 56402/56402 [14:30<00:00, 64.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2760 entries matching criteria\n",
      "Randomly sampling 1000 entries from 2760 valid entries\n",
      "Saved 1000 entries to filtered_quora.jsonl\n",
      "Average word count: 109.92\n",
      "Min word count: 100\n",
      "Max word count: 120\n",
      "Converted 1000 entries to CSV format\n",
      "Saved to filtered_quora.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a Jupyter Notebook\n",
    "\n",
    "# Configuration parameters\n",
    "input_file = \"Quora.jsonl\"  # Replace with your input file path\n",
    "output_file = \"filtered_quora.jsonl\"  # Replace with desired output file path\n",
    "min_words = 100  # Minimum word count\n",
    "max_words = 120  # Maximum word count\n",
    "max_entries = 1000  # Maximum number of entries to collect\n",
    "\n",
    "# Run the filtering process\n",
    "filter_dataset(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    min_words=min_words,\n",
    "    max_words=max_words,\n",
    "    max_entries=max_entries\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1000 entries to CSV format\n",
      "Saved to filtered_quora.csv\n"
     ]
    }
   ],
   "source": [
    "csv_df = convert_to_csv(output_file, \"filtered_quora.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
