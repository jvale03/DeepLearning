{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(144)\n",
    "random.seed(144)\n",
    "tf.random.set_seed(144)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../datasets/final_dataset.csv\")\n",
    "\n",
    "# First split: train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"Text\"], dataset[\"Label\"], test_size=0.2, random_state=144, stratify=dataset[\"Label\"]\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=144, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=100)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tumab\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/82\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5227 - loss: 0.6906"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=50, input_length=100),\n",
    "    SimpleRNN(64, activation=\"relu\"),\n",
    "    \n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8593 - loss: 0.3401\n",
      "Test accuracy: 0.8705\n",
      "\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8876 - loss: 0.2670\n",
      "Validation accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Accuracy: 0.7333\n",
      "\n",
      "Misclassified Samples:\n",
      "       ID Label_predicted  Prediction Label_actual\n",
      "1    D1-2           Human    0.013412           AI\n",
      "7    D1-8           Human    0.143792           AI\n",
      "9   D1-10           Human    0.466122           AI\n",
      "11  D1-12           Human    0.399194           AI\n",
      "19  D1-20           Human    0.135811           AI\n",
      "21  D1-22           Human    0.171110           AI\n",
      "25  D1-26           Human    0.453890           AI\n",
      "29  D1-30           Human    0.031945           AI\n"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "new_data = pd.read_csv(\"../datasets/dataset1_inputs.csv\", delimiter=\"\\t\") \n",
    "\n",
    "# Tokenize and pad the new data\n",
    "X_new_seq = tokenizer.texts_to_sequences(new_data[\"Text\"])\n",
    "X_new = pad_sequences(X_new_seq, maxlen=100)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new).flatten()\n",
    "\n",
    "# Convert predictions to labels\n",
    "labels = [\"AI\" if pred > 0.5 else \"Human\" for pred in predictions]\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\"ID\": new_data[\"ID\"], \"Label\": labels, \"Prediction\": predictions})\n",
    "\n",
    "# Load the correct labels (ground truth)\n",
    "ground_truth = pd.read_csv(\"../datasets/dataset1_outputs.csv\", delimiter=\"\\t\")  # Ensure it's tab-separated\n",
    "\n",
    "# Merge predictions with ground truth\n",
    "comparison_df = output_df.merge(ground_truth, on=\"ID\", suffixes=(\"_predicted\", \"_actual\"))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (comparison_df[\"Label_predicted\"] == comparison_df[\"Label_actual\"]).mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Show misclassified samples\n",
    "misclassified = comparison_df[comparison_df[\"Label_predicted\"] != comparison_df[\"Label_actual\"]]\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sentences Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Sentence: The question is meaningless because it assumes the existence of “creation scientists”. So-called creation scientists are not scientists. They are people pretending to be scientists to give themselves credibility with rubes who prefer a belief in magic to an acceptance of reality. Creation scientists are con men. Fraudsters. Liars. Some of them even have degrees in some scientific discipline, but it’s not likely to be biology. They have never published their findings in peer-reviewed scientific journals because they’re peer-reviewed, and that’s a filter they can’t pass through.\n",
      "Predicted Label: Human\n",
      "Prediction Score: 0.0415\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded sentence\n",
    "hardcoded_sentence = \"The question is meaningless because it assumes the existence of “creation scientists”. So-called creation scientists are not scientists. They are people pretending to be scientists to give themselves credibility with rubes who prefer a belief in magic to an acceptance of reality. Creation scientists are con men. Fraudsters. Liars. Some of them even have degrees in some scientific discipline, but it’s not likely to be biology. They have never published their findings in peer-reviewed scientific journals because they’re peer-reviewed, and that’s a filter they can’t pass through.\"\n",
    "\n",
    "# Tokenize and pad the sentence\n",
    "X_hardcoded_seq = tokenizer.texts_to_sequences([hardcoded_sentence])  # Use the same tokenizer\n",
    "X_hardcoded = pad_sequences(X_hardcoded_seq, maxlen=100)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(X_hardcoded).flatten()[0] \n",
    "\n",
    "# Convert prediction to label\n",
    "label = \"AI\" if prediction > 0.5 else \"Human\"\n",
    "\n",
    "# Print result\n",
    "print(f\"Sentence: {hardcoded_sentence}\")\n",
    "print(f\"Predicted Label: {label}\")\n",
    "print(f\"Prediction Score: {prediction:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
